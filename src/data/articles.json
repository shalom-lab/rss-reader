[
  {
    "id": "ruanyf",
    "title": "科技爱好者周刊（第 342 期）：面试的 AI 作弊----用数字人去面试",
    "description": "这里记录每周值得分享的科技内容，周五发布。\n本杂志开源，欢迎投稿。另有《谁在招人》服务，发布程序员招聘信息。合作请邮件联系（yifeng.ruan@gmail.com）。\n封面图\n\n四川省彭州市的中国蔬菜博览馆，设有一个\"种子银行\"，保存了200多种蔬菜的种子。（via）\n面试的 AI 作弊：用数字人去面试\n大家肯定想到了，AI 的能力现在这么强，一定有人用来作弊。\n求职作弊是最简单的，求职信和履历都可以让 AI 生成，但是你未必能想到，面试也能 AI 作弊，派一个数字人来面试。\n\"数字人\"技术几年前就有了，现在添加了 AI，简直如虎添翼，可以乱真。\n网上有很多教程，教你怎么生成数字人，哪怕不懂软件，都可以跟着做出来。\n\n\n\n只要上传脸部照片和语音片段，AI 就会生成你的数字化身，它跟你长得一模一样，用你的声音和表情说话。你让它说什么，它就说什么，就像下图这样。\n\n现在，很多公司采用视频面试，尤其是招聘远程员工，可能只有视频面试，根本没有线下面试。\n数字人既然跟真人长得一样，当然可能冒充真人参加视频面试。\n最近，网上就爆出了一个真实的案例，波兰的一家创业公司遇到了数字人参加面试。\n这个叫做 Bratislav Rakočević 的应聘者，有着非常详细完整的 LinkedIn 主页（下图）。\n\n简历也毫无问题（下图），看上去很有说服力，罗列了掌握的前后端技能，申请职位是全栈工程师。\n\n顺理成章，他就进入了视频面试。面试官见到本人（下图右）时，就觉得跟网页头像（下图左）不太像。\n\n而且，他使用了背景滤镜，整个人的影像是提取出来的，贴在背景上，头部边缘显得模糊而不自然。面试官当时也没有多想。\n他的面试表现极佳，任何问题都能快速应对，在规定的2小时内完成了所有编码题目和后续提问，这是前所未有的。\n但是，在交谈过程中，面试官逐渐产生了怀疑。首先，按照简历，这个人在塞尔维亚读大学，但是却不会说塞尔维亚语，只会说英语。（事后推测，原因很可能是，他的语音引擎没有塞尔维亚语，或者不如英语逼真。）\n其次，他的英语缺乏语气语调变化，说话没有沟通技巧，让人感觉有点机械。\n最后，他对以前职位的细节，说得不清楚，难以令人信服。\n为了测试这是否真人，面试官临时加了一个项目。\n\n面试官做了一个示范动作，举起一只手，张开五个手指，挡住自己的脸部，要求应聘者照着做（上图）。\n结果，应聘者说了一堆理由，拒绝了这个要求。至此，面试官确定，对面是一个 AI 数字人。\n他们把这件事公开到网上，希望其他公司提高警惕，不要被骗了。\n这样的数字人面试，以后肯定越来越多，怎么应对呢？\n下面是一些破解方法。\n（1）要求应聘者给出推荐人，以供联系核对。\n（2）查找应聘者的网络活动痕迹。如果网上根本搜不到什么痕迹，就说明很可疑。\n（3）除了视频面试，再安排一场线下面试。\n（4）在视频面试中，要求应聘者做一些数字人无法处理的事情，看看有没有破绽。比如，站起身围绕摄像头转一圈，头部前后左右转动，以及举起手做一些动作。\nAI 编程创意挑战赛\n不知不觉，稀土掘金和 Trae 联合举办的 AI FOR CODE 创意挑战赛，已经赛程过半。\n（1）\"创意赛道\"的提交入口，将在下周五（3月28日）截止。如果有好的 AI 编程创意（不需要动手实现），现在还能提交。\n（2）创意赛道的投票正在进行中，参赛者的名次通过投票获得，想得奖的同学要积极拉票呀。\n（3）\"应用赛道\"的提交入口，本周已经开启，大家可以提交自己的 AI 编程作品了。截止时间是3月31日，务必抓紧。\n\n本次大赛设有众多奖品，包括奖金、iPhone 16、华为mate 70、大疆pocket3、索尼PS5、外星人键盘&显示器、任天堂switch、马歇尔音响等，参赛即有机会抽奖。\n获奖的创意和作品，还可能获得投资公司的青睐，并且通过 AI 的助力，说不定可以解决很多人生活中面临的困境。\n大赛的详细介绍，以及提交/投票入口，请点击这里，或者扫描上方海报。祝愿大家都能得奖。\n科技动态\n1、特斯拉的自动驾驶\n特斯拉的自动驾驶，完全依靠摄像头，没有激光雷达。也就是说，它只有视觉识别。\n一位美国工程师认为，这样是不够的。他做了一个实验，在道路中央架起了一块巨大的画布。\n\n掀起遮盖后，画上是延伸到前方的道路，跟周围融为一体。\n\n结果，特斯拉的自动驾驶，根本认不出来这是画，一头就撞上去了。\n\n相比之下，使用激光雷达的车型，则会在画前自动停下来。\n虽然这个实验是极端情况，但是摄像头在浓雾和大雨天气，效果不佳，却是不争的事实。\n2、Windows 记事本的收费功能\n记事本（Notepad）和画板（Paint），一直是 Windows 的基本组件，每一代 Windows 都内置了，免费使用。\n但是，Windows 11 最新一次的更新，为这两个软件加入了收费功能。\n\n微软为记事本和画板配置了 AI，前者可以自动重写、缩写、扩写文章，后者可以自动生成图像。\n这些 AI 功能只有 Microsoft 365 的订户才能使用（月费9.99美元）。如果没有付费，就无法使用，右上角的 AI 菜单会变灰色（上图）。\n从此，记事本和画板不再是免费软件了，部分功能有付费墙。\n3、AI 去除图像水印\n很多美国用户在社交媒体上反映，谷歌新发布的 Gemini 2.0 Flash 模型，去除图片水印的效果极佳。\n这是带有水印的原图。\n\n这是大模型去除水印的效果。\n\n该模型不仅能去除水印，还能填补去除水印而产生的任何空白。\n\n\n虽然其他模型也能去除水印，但是 Gemini 2.0 Flash 似乎特别擅长这件事，而且它可以免费使用。\nGemini 2.0 Flash 可以在谷歌官网使用。另外，网友 @panjianning 投稿了一个自己做的调用地址。\n4、其他\n（1）中国首款超长寿命碳-14核电池\"烛龙一号\"研制成功，利用同位素衰变供电，理论上可以持续放电上千年，适合高山海洋、宇宙空间、脑机接口、心脏起搏器等场景。\n\n（2）美国劳工局统计，美国在职的程序员目前有30多万，是1980年以来的最低值，仅为21世纪初互联网繁荣时期的一半。\n原因可能是 AI 的冲击，仅仅2023年一年，程序员就业人数就急剧下降了27.5%。\n（3）Nginx 服务器拒绝为默认的404页面，添加暗黑模式（下图右），理由是不愿增加复杂性，而且用户完全可以自定义这个页面。\n\n（4）谷歌 AI 部门负责人称，通用人工智能（AGI）将在未来五到十年内出现。\n文章\n1、服务器发送事件（SSE）被低估了（英文）\n\n本文详细介绍\"服务器发送事件\"是什么，以及目前的 AI 服务如何使用它。\n2、关于继承和子类型（英文）\n\nJava、Go、Python 这三种语言，有不一样的继承设计，本文比较了它们各自如何实现一个子类型。\n3、CSS 属性的 unset 值（英文）\n\nCSS 属性几乎都可以设成三个值 initial（不继承）/inherit（继承）/unset（取消默认值），本文用例子解释它们的含义。\n4、布尔属性的陷阱（英文）\n\n本文提出一个编程技巧：尽量少在类（class）里面设置布尔属性，而要改用枚举（enum）。\n5、我从零制作了一个智能手表（英文）\n\n作者介绍怎么制作一款智能手表，可以用来了解硬件工程师的工作。\n6、:user-valid 伪类（英文）\n\n一篇 CSS 教程，介绍 :user-valid 伪类（表示用户的输入通过了 input 输入框的校验）。\n7、使用 GitHub Actions 和 GitHub Pages 构建和部署网站（英文）\n\n本文是一篇教程，介绍如何使用 GitHub Actions 构建网站，然后部署到 GitHub Pages。\n工具\n1、daylight\n\n命令行查询日出日落时间，可以指定地点和日期。\n2、FilePizza\n\n在浏览器里\"点对点\"传送文件的开源工具。\n3、git-who\n\n一个开源的命令行工具，显示 Git 仓库的提交者统计。\n4、rust-stakeholder\n\n一个命令行工具，唯一作用就是让你的终端显得很忙，源源不断有输出，适合摸鱼。\n5、hoarder\n\n一个自托管的书签应用，有 Web 端和手机端，可以用 AI 自动生成内容标签。\n6、微信群聊的永久二维码\n\n微信群聊的二维码频繁变动，这个工具可以生成永久二维码，基于 Cloudflare Workers 和 KV 存储。（@xxnuo 投稿）\n7、Webcam Runner\n\n一个对着电脑摄像头的开源跑步游戏，检测用户动作来控制游戏角色在无限场景中奔跑，适合室内锻炼身体。（@Jamesun921 投稿）\n8、Cover Page\n\n免费的封面图制作网站。（@amuluze 投稿）\n9、EchoShare\n\n开源的基于 WebRTC 的在线屏幕共享工具，允许与他人共享屏幕、摄像头和音频。\n（@shawroger 投稿）\n10、Lazyeat\n\n开源的 Windows 桌面应用，手势控制电脑。（@maplelost 投稿）\nAI 相关\n1、SVG 秀\n\n根据文字描述，生成 SVG 图片，并可以编辑，代码开源（前端和后端）。（@chaseFunny 投稿）\n2、MarkPDFDown\n\n基于大模型的 PDF 转 Markdown 工具，实现文档结构化转换。（@jorben 投稿）\n3、We0\n\n通过 AI 生成应用程序，支持后端生成和前端生成，还可以 Sketch/Figma 设计稿1:1还原，代码开源。（@Mashiro2000 投稿）\n4、Githubhunt\n\n使用自然语言，搜索 GitHub 仓库。（@xgzlucario 投稿）\n5、Prompt Optimizer\n\n开源的 AI 提示词优化工具。（@linshenkx 投稿）\n6、Bob plugin MTranServer\n\nBob（macOS 平台的翻译软件）的一个插件，引入本地翻译服务器 MTranServer。（@gray0128 投稿）\n资源\n1、Maple Mono\n\n开源的等宽字体，支持中文。（@subframe7536 投稿）\n2、上网2005\n\n还原20年前的中文互联网。（@wong2 投稿）\n3、Bambot\n\n网友开源的低成本（约2000元人民币）的人形机器人。（@timqian 投稿）\n4、OpenAPK\n\n这个网站提供开源的安卓 App 下载。\n图片\n1、极光月全食\n上个月，一个美国摄影师前往阿拉斯加州拍摄极光。\n他无意中发现，这段时间还有月全食，于是成功拍到了极光中的月全食。\n\n上面照片中，右下角的红点就是月球。\n\n月全食的时候，月球、地球、太阳成一条直线，月球落在地球的阴影里面，照不到直接的太阳光，而是被地球大气层反射的太阳光照亮。\n地球反射的是太阳光的红光，所以月全食呈现红色，又称\"血月\"。这张绿色极光中的\"血月\"照片，非常难得。\n2、EK215 航班路线\n地球是圆的，但是世界地图是采用\"墨卡托投影法\"绘制的平面地图，它会让高纬度地区被大大拉长，变形严重。\n阿联酋航空 EK215 航班，从迪拜飞往洛杉矶，下图是它的航线在地球仪上的样子，以及在平面地图上的样子。\n\n可以看到，在地球仪上，这条航线基本是直的，路线非常合理。\n但是，在平面地图上，北极地区的航线被拉长成了一条巨大的弧线，越靠近北极，航线的变形就越严重。\n文摘\n1、耐力是一种优势\n人们常常低估耐力的作用，只把它理解成比别人更努力、更持久。\n实际上，耐力也是坚守自己的价值观和目标的能力，即使在看上去很难做到的时候，也不放弃。\n在缺乏明显进展的情况下，坚持去实现目标的能力，这就是耐力。\n在一个充满诱惑分散你注意力的世界中保持专注，克服困难，继续前进，你需要耐力。\n耐力是人们可以培养的最有用的品质之一。它比力量、智力、速度、魅力等特质，具有更大的适用性，日常生活很多时候都能用到。\n更聪明的人某些时候会表现出色，比你更快地解决难题。但只要凭着耐力，你可以解决更多的问题。\n言论\n1、\n开源运动的人们完成了不可能的任务。他们创造了整个百科全书、地球上最成功、使用最广泛的操作系统、软件库和无数应用程序。他们对公共资源的贡献甚至在科幻小说中都难以想象，其中一些系统应该被视为世界的数字奇迹。\n-- 《自由软件为了谁？》\n2、\n你的应用最好不要依赖云服务商，要做到\"可弹出\"（ejectable），即随时可以切换到自托管环境。\n你的应用应该有一个 workspace.zip 文件，里面包含了当前的所有状态。只要转移这个文件，就可以还原当前状态。\n--《本地优先且可弹出》\n3、\n我见过的最好的工程师，是那些愿意在周末花几个小时构建一个现有软件的自己版本的人。\n这就是你获得创新和进步的方式。如果你不了解系统的工作原理，就无法找到改进的地方。\n-- 《AI 让开发者变蠢》\n4、\n使用 GitHub Copilot 后，我得了一种叫做\"Copilot 延迟\"的病。这种病指的是工程师在每次操作后都会暂停，等待 AI 提示他们下一步该做什么。\n很多工程师有了 AI 以后，就做不到只靠自己了，要靠 AI 告诉他们下一步。这类似于初级程序员在刚开始时，依靠资深的同事的指导开展工作。\n-- 《AI 让开发者变蠢》\n5、\n如果你成功了，记住你要去哪里，记住你来自哪里，并选择你要成为什么样的人。\n-- 《五周的独自创业》\n往年回顾\n巧妙的灯泡钟（#295）\n摩天大楼是反人类的（#245）\n你做过不在乎结果的项目吗？（#195）\n大家不出门，经济怎么办？（#145）\n（完）\n文档信息\n版权声明：自由转载-非商用-非衍生-保持署名（创意共享3.0许可证）\n发表日期： 2025年3月21日",
    "link": "http://www.ruanyifeng.com/blog/2025/03/weekly-issue-342.html",
    "pubDate": "2025-03-21T00:13:43.000Z",
    "source": "阮一峰的网络日志",
    "category": "技术博客"
  },
  {
    "id": "ruanyf",
    "title": "科技爱好者周刊（第 341 期）：低代码编程，恐怕不会成功",
    "description": "这里记录每周值得分享的科技内容，周五发布。\n本杂志开源，欢迎投稿。另有《谁在招人》服务，发布程序员招聘信息。合作请邮件联系（yifeng.ruan@gmail.com）。\n封面图\n\n成都出现了花田火锅，火锅店开在盛开的油菜花地里，运营期两个月。（via）\n低代码编程，恐怕不会成功\n这十几年，一批批程序员前仆后继，去搞低代码编程（包括无代码编程）。光在我身边，就有三四批。\n\n他们搞的低代码编程，我理解就是通过图形界面，拖拉各种组件，自动生成软件 UI 的底层代码，减少手工编码。\n\n这个想法很好，确实很多人需要，尤其不懂编程的人，这简直是生成程序的唯一可用方式。\n但是很奇怪，他们无一例外都失败了，开发出来的低代码工具，开始还有一些好奇的用户，很快就不来了，用户越来越少，后来即使开源了，也没人用。\n更奇怪的是，这似乎不是偶然现象，业界所有的低代码工具好像都不成功，至少我想不出成功的例子，哪一个受欢迎的应用程序是用低代码工具生成的。\n它的背后有什么原因吗？是哪里没有做对，还是低代码编程本身就不可行？\n我一直没有想通这个问题。虽然不看好，但是依然抱有一丝幻想，也许某一天醒来，低代码编程就成了主流，无论手机 App 还是桌面应用，鼠标拖几下，就可以生成。\n\n直到上周，我读到一篇文章《低代码编程受困于形式》（下图），才恍然大悟，低代码编程有先天缺陷，恐怕不会成功。\n\n文章说，优秀的作品都是形式（form）和功能（function）的统一。形式必须服从功能，功能决定了形式，英文叫做\"form follows function\"。\n\n对于优秀的程序员，只要弄清楚了底层，UI（用户界面）就会显而易见。\n低代码编程的问题在于，它是先有 UI（形式），再有代码（功能）。\n用户先拖拉生成 UI，系统再根据 UI 生成代码。这是本末倒置，让底层代码适配 UI，注定了两者都有问题：UI 是空想出来的，代码为了适配 UI，注定冗余和低效。\n所以，优秀的软件不可能用这种方式生成，低代码编程不会成功。\n我认为，他说的很有道理。低代码编程解决不了这个根本缺陷，适用场景有限，大概只适合一些简单任务，或者生成原型，不会成为主流工具。程序员应该谨慎开发这类工具，付出的劳动很可能打水漂。\n写到这里，问题就来了：AI 算不算低代码编程（或者无代码编程）？如果低代码编程不会成功，那么 AI 编程会成功吗？\n我认为，AI 不同于低代码编程。低代码编程是使用者给出 UI，系统来生成代码，而 AI 是系统同时生成 UI 和代码，用户只需要说出需求即可。\n这种情况下，形式与功能的结合，完全取决于 AI 的能力。如果有一天，AI 视频能够成功，画面美，情节好，那么 AI 编程大概也会成功，生成形式与功能统一的应用程序。\n小程序容器 FinClip\n现在的手机 App 有一个技术趋势，大家注意到了吗？\n那就是添加小程序容器，让自家 App 能够运行其他应用程序。\n不仅国内 App 这样做，海外的一些超级 App 也纷纷效仿，比如 YouTube、Telegram、Line。\n\n上图就是 Youtube 应用内置的\"小游戏\"，类似于微信小程序。\n究其原因，大概是因为小程序这种架构很灵活，可以方便地添加和更新功能，有利于形成外部生态和变现。\n今天，就向大家介绍一款国产的小程序容器 FinClip。如果你想为自己的 App 引入小程序，就用得到它。\n它是一个完整的、开箱即用的小程序技术解决方案，提供现成的 SDK，将小程序运行环境嵌入宿主 App。\n有了它，任何开发者都能在 iOS / Android / HarmonyOS 等平台，构建自己的\"小程序宇宙\"。下图是 FinClip 目前支持的宿主平台。\n\n可以看到，除了手机系统，它还支持嵌入桌面应用、车机应用、电视应用等等。\n此外，它还有一些很吸引人的技术特性。\n（1）跨平台统一性。它直接兼容微信/支付宝/抖音小程序，可以一行不改，直接让微信小程序跑在你的应用里面，无需二次开发。\n（2）敏捷开发范式。它的小程序更新无需应用商店审核，可以实现\"小时级\"迭代。\n（3）小游戏引擎。跟它配套的还有一个小游戏实时内容互动引擎 RealClip，提供小游戏运行环境。\n\n这个引擎重点针对小游戏的性能和兼容性，进行了优化，除了微信小游戏，还兼容 Unity、Cocos、Laya、Egret Engine 等主流引擎和 WebView。也就是说，无论哪种引擎制作的小游戏，都能够直接运行在你的 App 中。\n（4）配套开发工具 FinClip Studio。这个工具能将现有的小程序/小游戏代码，一键转换成独立 App，做成可用于 iOS、Android 和鸿蒙的安装包。\n总之，如果你的 App 想引入小程序，或者你现有的小程序需要运行在其他应用（手机或桌面），那么就可以尝试 FinClip。\n欢迎访问 Finclip 官网详细了解，免费注册试用。任何问题都可以加入官方社群交流（下图）。\n\n[活动] AI 创意挑战赛，开始投票了\n上周五，周刊发布了消息。首届全国 AI 编程大赛的\"创意赛道\"，可以提交作品了，不管你会不会编程，只要有 AI 创意，都可以参赛。\n消息发布后，大家反响热烈。我从组委会同学那里得知，已经有几百个创意提交了。\n\n从今天（3月14日）开始，大赛进入了投票阶段，欢迎大家投票，评出最能打动你的创意。也许你还可以从中得到灵感，拿来自己实现。\n所有投票用户均可抽奖，奖品有小夜灯、工卡套、手持风扇等等。因为投票是当天有效，第二天可以再次投票，所以每个人有多次抽奖机会。\n当然，如果你还有创意没有提交，现在依然可以提交参赛。\n已经参赛的同学，不要忘了为自己拉票，发布到社交平台上（公众号/b站/小红书/微信朋友圈/技术社群），分享比赛，争取大赛获奖。\n大赛详情和作品投票，可以点击这里，或者扫描上方海报。\n科技动态\n1、人工心脏\n一个澳大利亚男子，植入了一颗人工心脏（下图），已经活了100天，并且成功出院，创造了世界纪录。\n\n这相当于在胸腔植入一个血液泵，一天24小时推动血液循环。\n他是目前世界唯一一个带有人工心脏的人，也是世界第六例人工心脏植入。前五例的人工心脏都只是过渡，病人后来又移植了其他人的心脏。\n如果机器心脏以后技术成熟了，人类的寿命可望大幅延长。\n2、手机改路由器\n旧手机有什么用？\n一家比利时公司取出手机主板，加上网线口、USB 口，将其改成了路由器。\n\n上图左侧是手机主板，右侧是将手机主板安装在扩展板上，从而形成路由器主板。\n手机的 CPU、调制解调器、内存等，都是可复用的，因此改装费用很低。而且，手机的硬件配置很高，性能比高端路由器强得多。\n3、静音图标\n苹果上周发布的 MacBook Air M4，悄然改掉了一个26年之久的设计。\n它把笔记本的静音图标（F10 按钮上的图标），在喇叭上加了一道删除线。\n下图是以前的图标。\n\n下图是现在的图标。\n\n这么简单的一个图标，苹果用了26年才改掉。\n4、汽车的物理按钮\n德国大众汽车宣布，未来它的所有车型，都会同时配备触摸屏和物理按钮。\n\n上图是大众 ID.3 车型的控制台，上方是触摸屏，下方都是按钮。\n该公司表示，汽车不是手机，不能都靠触摸屏，重要的功能必须有固定的位置和真实的触感。\n5、其他\n（1）一家英国生物公司，研发了转基因香蕉。这种香蕉可以长期保持新鲜和黄色，不会变褐变黑。\n\n即使剥开后12小时，香蕉皮也不变色，这样有利于香蕉销售。\n（2）Android 15 将有一个原生的终端程序，提供一个基于 Debian 的 Linux 发行版供用户使用。\n\n这个功能的底层是虚拟机机制，它将大大方便程序员，将安卓手机当作 Linux 桌面电脑使用。\n\n文章\n1、AI 大模型2024年的进展（英文）\n\n著名程序员 Simon Willison 3月7日的演讲稿，通俗地介绍了 AI 在过去一年的巨大飞跃，很好的综述。\n2、如何用 Claude Code 反编译代码（英文）\n\n作者演示了一个惊人的例子，使用 Anthropic 发布的 Claude Code，将 Webpack 编译出来的文件反编译，还原成源代码。\n3、CSS 跨文档视图转换（英文）\n本文介绍一个示例，通过 CSS 新的跨文档视图转换功能，使得多页面应用的跳转，也像单页面应用（SPA）一样流畅顺滑。\n4、Cursor 上传 .env 文件（英文）\n\nCursor 是现在非常流行的一个 AI 代码编辑器，它的用户论坛爆出一个帖子，有人发现它会上传用户的 .env 文件，由于里面都是环境参数，会带来安全隐患。\n5、JSON 与 JavaScript 的对象成员顺序（中文）\n\nJSON 与 JavaScript 的对象，里面的成员顺序有没有规定？本文探讨这个问题。\n6、Go 语言错误处理机制的优点（英文）\n\nGo 语言的错误处理很特别，没有 try...catch 机制，错误是一个值，作者解释这样设计的好处。\n工具\n1、TypeScript 7\n微软使用 Go 语言重写了 TypeScript 编译器 tsc，据说速度可以提高10倍，参看介绍文章。\n\n目前，TypeScript 的版本是5.8，等到这个工具稳定了，将发布为 TypeScript 7。\n2、QR Code Generator\n\n一个网页应用，可以定制二维码的颜色、斑点、徽标。\n3、WatchYourLAN\n\n一个开源的网页应用，用来扫描局域网的 IP 分配，可以发送主机上线和掉线的通知。\n4、XPipe\n\n一个跨平台的桌面应用，通过图形界面，将所有的服务器连接在一个地方管理。\n5、TransBridge\n\n一个开源的翻译 API 代理服务，可以接入各种大模型，对外提供翻译服务，试用 Demo。（@fruitbars 投稿）\n6、DouYin Downloader\n\n开源的 Python 脚本，用来下载抖音短视频。（@jiji262 投稿）\n7、Java Thread Dump\n\n免费分析 java thread 的网站，上传 jstack 导出的线程快照文件，分析线程池内线程状态。（@HbOrea 投稿）\n8、Mono\n\n制作内容分享卡片的免费网站。（@RiverTwilight 投稿）\n9、Telegram Files\n\n开源的 Telegram 文件下载器，支持多频道、多账户同时下载。（@jarvis2f 投稿）\n10、Obsidian 云盘同步插件\n一个开源的 Obsidian 插件，将笔记自动同步到多种云盘服务。（@ai-bytedance 投稿）\nAI 相关\n1、Mistral OCR\n上周，Mistral AI 发布了号称史上最强的 OCR 识别工具，具有公式和表格的识别能力，参见介绍文章。\n\n网友 monsoonw 做了一个免费的试用网站。\n\n著名程序员 Simon Willison 开源了一个 Python 脚本，演示了怎样调用 Mistral 的 API 进行文字识别，参考他的文章。\n2、Free QWQ\n\n免费、无限制的算力平台，为开发者提供 QwQ 32B 大语言模型 API。（@nexmoe 投稿）\n3、Code-Review-LLM-Gitlab\n\n使用大模型对 GitLab 项目进行 Code review 的工具。（@mimo-x 投稿）\n4、人话翻译器\n一个 Chrome 插件，通过 AI 将难懂的中文翻译成好懂的中文。（@DemoJ 投稿）\n资源\n1、辰宇落雁体\n\n一个开源的中文手写字体。\n2、JetBrains Maple Mono\n\n一款合成字体，解决 JetBrains Mono 没有中文字形的痛点，全部等宽无衬线，中英文 2:1 宽对齐。（@SpaceTimee 投稿）\n3、BeddyStories\n\n一个儿童睡前故事网站，收集了全球经典的儿童睡前故事。（@yimiqidage 投稿）\n4、IP 侦探\n\n免费的在线 IP 归属地查询。（@Oliverwqcwrw 投稿）\n另有一个 Chrome 插件 IP Location Finder，选中 IP 地址，显示归属地。（@Yanel85 投稿）\n\n图片\n1、红绿色盲\n红绿色盲的患者，看不到红色和绿色。在他们眼里，这两种颜色都会变成黄色。\n下面是一半红、一半绿的树叶。\n\n红绿色盲患者看到的却是一张黄色树叶。\n\n大概每20个人里面，就有一个人有色盲或色弱问题。所以，设计界面的时候，使用红色或绿色必须非常谨慎，因为红绿色盲患者分不清。\n下面的日历使用绿色和粉红色，表示特殊的日期。\n\n但是，红绿色盲患者看到的是下面这样，根本分不清。\n\n因此，用户界面轻易不要使用红色和绿色。\n文摘\n1、如果 AI 和机器人接管一切\n我最近常常想一个问题：如果 AI 强大到所有方面都超过人类，它和机器人接管一切，人类要干什么呢？\n凯文·凯利认为，随着工作都交给机器人，人类可以从事越来越多有趣的工作，就像工业革命后一样。\n这种说法在短期内有一定道理，但是有一个前提，就是人类能做计算机做不到的事情。\n我认为，没有理由认为这个前提会永远成立。\n除非政府强制规定，计算机不得从事某些工作，只有人类可以做。但是那样的话，那些工作很可能就会停滞发展了。停滞发展的行业没有前景，收入也不会增长，从业者难以感到满意。\n让我们假设一种极端的情况，如果机器完全超越人类，每件事都比人类做得好，大部分人无法为社会做出贡献时，一切会怎样？\n如果一个人无法为社会做出贡献，也就失去了他的经济价值，就算他能靠政府的补助继续活着，那么对于他来说，个人价值是什么呢，就是活一天算一天？\n目前来看，这个问题还比较遥远，就算那一天到来，也是很久以后的事情了。眼下比较现实的问题是，AI 正在大量减少高薪工作。随着机器的能力越来越强，很多白领工作的价值迅速变小，大多数人越来越难找到报酬丰厚、令人满意的工作。\n这就是现在发生的问题，高薪的工作岗位不断减少，难以获得。\n言论\n1、\n以前的球票、音乐会票、景点票、电影票都是纸质的，现在全改成数字的。\n我们的过去都保存在手机里，再也没有纪念物了。\n-- 彭博社\n2、\n越来越多的应用程序转移到互联网上，操作系统的软件差异变得越来越不重要。这就是为什么 M1 芯片对 Mac 的未来如此重要。\n苹果应当利用这一波 AI，发挥其硬件优势，鼓励开发者在本地运行 AI 模型。\n-- 《苹果 AI 的潜力》，本文指出苹果芯片可以本地运行 AI 模型，不需要 Nvidia 显卡，苹果应该利用这一点，扩大销售\n3、\n自从有了 AI，我发现自己不再担心项目对我来说太大、太复杂，或者项目使用了我不了解的技术或编程语言，一切都变得容易得多。\n我正在重新审视一些我曾认为太复杂或超出我能力范围的业余项目，只要有时间，我就会去尝试。这是一个令人兴奋的时代。\n-- 《有了 AI，你需要想得更大》\n4、\n我认为，数学本质上已经没有什么好问题了。让大量数学家感兴趣的问题数量每年都在减少，而且几乎所剩无几。\n现代数学研究越来越局限于少数人对某个特定主题的研究，即使是研究生也常常被现代数学问题的极端专业性和深奥性所困扰。\n未来的研究生不应再需要证明一些全新的东西，相反地，他们的主要目标可能是简化过去的研究结果。\n-- 《数学已经没有问题了》\n往年回顾\n崖门海战的感想（#294）\n大数据已死（#244）\n悲观者正确，乐观者成功（#194）\n提高收入的根本途径（#144）\n（完）\n文档信息\n版权声明：自由转载-非商用-非衍生-保持署名（创意共享3.0许可证）\n发表日期： 2025年3月14日",
    "link": "http://www.ruanyifeng.com/blog/2025/03/weekly-issue-341.html",
    "pubDate": "2025-03-14T00:08:44.000Z",
    "source": "阮一峰的网络日志",
    "category": "技术博客"
  },
  {
    "id": "ruanyf",
    "title": "科技爱好者周刊（第 340 期）：技术炒作三十年",
    "description": "这里记录每周值得分享的科技内容，周五发布。\n本杂志开源，欢迎投稿。另有《谁在招人》服务，发布程序员招聘信息。合作请邮件联系（yifeng.ruan@gmail.com）。\n封面图\n\n成都建筑师刘家琨，本周获得号称\"建筑界诺贝尔奖\"的普利兹克奖，上图是他的作品苏州御窑金砖博物馆。（via）\n技术炒作三十年\n大家有没有发现，每隔一段时间，媒体就会大肆炒作一种新技术，宣扬它将对人类产生巨大影响，全社会都在关注，人人都在谈论。\n\n这种炒作就是大家常说的\"风口\"吧。突然之间，风就起来了，如果正好站在风口，猪也能飞起来。\n你能举出多少个这种炒作的例子？\n一个国外程序员根据回忆，列出了过去三十年主要的几次技术炒作。\n1998-2001 年：互联网 WWW\n1999-2006 年：Java\n2004-2007 年：Web 2.0\n2007-2010：云计算\n2010-2015：社交媒体\n2012-2015：物联网\n2013-2015：大数据\n2017-2021：区块链\n2021 年至今：人工智能\n大家觉得，这个时间列表是否准确？\n\n我的亲身感受是差不多。这是主要的几次技术炒作，而且这些技术都成功了，所以炒作的时间才会持续这么久，两年到五年，然后被下一个热点取代。\n当中，还有许多次小的技术炒作，但都没有那么成功，持续时间就没有这么久，很快退潮了，比如元宇宙、Web 3、AR/VR 眼镜、3D 打印、自动驾驶等等。\n一种新技术能够带来大规模、长时间的炒作，有一个前提条件，那就是它有真东西，确实能对社会经济带来非常有感的变化。\n上面列表的每一种新技术，确实都是大的突破，改变了技术方向，没有一个是虚的。如果再加上智能手机、短视频、加密货币，可能就把最近三十年大的技术\"风口\"都包括了。\n我以前有一个误区，看不起技术炒作，认为那只是一窝蜂的音浪，跟娱乐版的明星炒作没什么不同。\n人到中年，我才意识到，这种观念大错特错，技术从业者千万不能有这种想法。每一次技术炒作，不仅是音浪，更是机会，会带来空前的关注、疯狂涌入的资金、以及切切实实的需求。炒作规模越大、程度越厉害，带来的机会和资金也就越大。\n每一次大规模的技术炒作，都会诞生一些快速增长的指标公司，创造巨大的财富效应。如果你正好身在其中，事业和财富都会随之起飞。\n让我们现实一点，一个工程师最有技术生产力、创造力、事业起飞的时间窗口，就只有那么几年。如果个人事业要快速起来、为未来铺好道路，光有技术还不够，还必须赶上至少一个大的技术风口，用外部的资金和需求放大个人努力。\n否则，单靠自己的成果积累，就太慢了，很难快速到达更高的层次，很可能辛辛苦苦干了二十年，还是在做一些基础的事情。如果出现技术升级，使得你的技能过时了，后面的路就难了。技术风口其实是实现个人阶层飞跃、人生翻转的最可行的路径。\n所以，每一轮大的技术风口并不完全是一哄而上的炒作，里面包含了一些真正的机会，值得关注和跟上。这也是为什么周刊每一次都对新技术倍加关注、积极评价的原因。\n当然，赶上技术风口的前提，还是要有真才实学，能做出实打实的产品。否则，真遇到风口，你也无法脱颖而出，拿到技术炒作的红利。\n[活动] AI FOR CODE 创意挑战赛\n上周提到的全国 AI 编程大赛，大家还有印象吗。\n这次大赛为了让更多人参与，除了常规的\"应用赛道\"，还特别设置了\"创意赛道\"。\n只要有创意，就能参加，不需要具体的实现，实现交给 AI。\n\n创意赛道从今天（3月7日）开始，就可以提交作品了，到3月27日截止。\n如果你有想让 AI 实现的创意（点子），不妨发布到作品提交专区。发布时，需要按照模版要求提交。\n注意，发布的创意需要公开可见，这样才能让大家投票。得票高的创意，将有丰富的奖品。\n每个人最多提交5个创意。如有团队使用你的创意完成项目开发，你将获得神秘大礼！\n不要错过本次大赛，只要你有想法，就有机会得奖。提交创意和投票的详细介绍，可以点击这里，或者扫描上面海报的二维码。\n修复壁画的新方法\n意大利帕多瓦教堂，曾经有一幅巨大的中世纪壁画，非常精美。\n\n但是，这幅壁画在1944年的第二次世界大战被炸毁。\n下面是壁画的虚拟重建图（局部）。\n\n壁画被炸毁时，人们把墙壁的碎片收集起来，一共有88000多块，存放在博物馆。\n下面是碎片的照片，这些碎片大概只占原始壁画的10%。\n\n碎片的数量太大，缺失又太多，从来没人敢于尝试将它们还原。\n但是，1992年的时候，博物馆曾经将所有碎片拍成了数码照片。\n慕尼黑工业大学的数学教授马西莫·福纳西耶（Massimo Fornasier）得知了这件事，决定基于这些数码照片，使用计算机进行壁画还原。\n这幅壁画在战前，曾经有过一张黑白照片（下图），可以作为修复的依据。\n\n马西莫教授的第一步，是将这张照片上色，还原成彩色照片。\n\n然后，将碎片进行图像吻合，一块块找到它们的位置（下图）。\n\n下面是碎片上墙的样子。\n\n缺失的部分，团队用灰白颜色补全。\n\n全彩的虚拟效果图如下。\n\n科技动态\n1、美国萤火虫航天公司的\"蓝色幽灵\"登陆器，成功登陆月球，成为第一家登陆月球的民营企业。\n\n2、一家美国生物技术公司，改造了老鼠的毛发基因，成功培养出了长毛鼠。\n\n\n他们下一步的目标，是培育长毛象。\n3、本田公司推出一款口袋妖怪摩托车，外形非常惊艳（下图）。\n\n4、联想推出一款太阳能笔记本，上盖覆盖了太阳能电池。\n\n据说阳光下放20分钟，可以播放视频1小时。但是怎么看，都不如外接一个太阳能发电板实用。\n5、微软将在今年5月关闭通信服务 Skype，由 Teams 替代。智能手机出现之前，Skype 是最流行的国际电话软件。\n\n文章\n1、流式 HTML（英文）\n\nAI 的聊天对话都是流式加载的，本文介绍一个技巧，不使用 JS 也能加载流式内容。\n2、不要用 TypeScript 枚举（英文）\n\nTypeScript 官方已经不建议使用 enum（枚举）语法，作者建议改用字符串的联合类型代替。\n3、我如何使用 roboscribe 音频转文本（英文）\n\n一篇教程，作者使用软件 roboscribe 将播客转成可用的文本，这事要比听上去麻烦。\n4、如何测试电梯（中文）\n\n本文将电梯抽象成一个\"有限状态机\"，设计测试用例。（@lezhi12 投稿）\n5、我为什么选择 Firefox（英文）\n\n作者介绍 Firefox 浏览器胜过 Chrome 的几个地方，有些功能大家可能未必知道。\n6、脚本代替别名（英文）\n\n常用的终端命令，往往可以设置别名（alias）作为快捷方式，作者提出一种新的方式，用脚本代替别名，更容易维护。\n7、Tailscale 对我很有用（英文）\n\n作者介绍自己的 Tailscale 用法，将不同的设备组成一个虚拟局域网。\n工具\n1、Yaak\n\n一个测试 API 的开源桌面软件，功能比较全。\n2、cleanmac\n清理 macOS 系统的一个命令行脚本。\n3、Lynx\n\n字节开源的一款跨平台原生应用开发工具，使用 Web 语法，生成各个平台的原生应用，类似于 React Native。\n4、appstat\n\n监控 Windows 应用的资源占用（内存、CPU、网络）的一款工具。\n5、Maestro\n\n一个 Web 和 手机的 UI 测试工具，只要写好配置文件，就能自动运行测试。\n6、Git Worktree Manager\n\nVS Code 插件，方便在不同的 Git 仓库、不同的分支之间切换。（@jackiotyu 投稿）\n7、Hugo Translator\n一个 Python 脚本，将 markdown 格式的中文 Hugo 博客帖子，翻译成英文。（@Rico00121 投稿）\n8、O-Spy\n\n一个 Web 应用的记录并回放用户操作的工具，用来远程调试。（@wqcstrong 投稿）\n9、MTranServer\n\n开源的离线翻译服务器，号称资源消耗低，CPU + 1G 内存即可运行，支持调用沉浸式翻译。（@xxnuo 投稿）\n10、Screen Sharing Application\n\n一个开源的 Next.js 应用，通过点对点通信，实时分享你的屏幕。它会生成一个房间码，其他人访问这个房间，就能看到你的屏幕。\nAI 相关\n1、olmOCR\n\n一个使用 AI 模型进行文字识别（OCR）的 Python 工具。\n2、Probly\n\n一个基于 AI 的电子表格软件，可以在浏览器中对表格运行 Python 代码。\n3、Hacker News 每日播报\n\n每天自动抓取 Hacker News 热门文章，通过 AI 生成中文播客。（@Y024 投稿）\n4、语析\n\n基于大模型，进行知识库管理与生成知识图谱的工具。（@xerrors 投稿）\n5、DiffRhythm\n\n西北工业大学 ASLP 实验室开发的一个 AI 音乐生成模型。（@JoeDeanx 投稿）\n资源\n1、Meta 的 AI Demo\n\nMeta 公司的 AI 实验室，展示最新的成果。\n2、ProWords\n\n一个基于 AI 的单词记忆平台，根据职业身份生成例句，代码开源。（@winterfx 投稿）\n3、圣彼得大教堂 3D 导览\n\n梵蒂冈的圣彼特大教堂（St. Peter's Basilica）是世界最大教堂，这个网站提供它的 3D 模型还原。\n4、Shapecatcher\n\n这个网站根据你画出的形状，返回匹配的 Unicode 字符，包括 Emoji 字符和东亚文字。\n图片\n1、奥乐齐的条形码\n奥乐齐（Aldi）是一家德国连锁超市，为了方便用户扫描条形码，把条形码印刷得特别长。\n\n小包装商品无法放置那么长的条形码，奥乐齐就会设法放置多个条形码。\n\n上图的奶酪通心粉，在侧面和底部都有条形码。\n2、乐高日心仪\n国外网友使用乐高积木，搭建了一个可以转动的日心仪。\n\n上图中，中间黄色的是太阳，地球围绕太阳公转，并且有22.5°的倾斜角。\n\n地球的旁边还有月亮。月亮其实有5.15°倾角，但是肉眼不容易察觉。\n\n它是可以实际运转的，内部结构很复杂，有大量齿轮。\n\n\n文摘\n1、高管与普通员工的脱节\n作者：伊森·埃文斯（Ethan Evans）\n我是已经退休的亚马逊副总裁，在亚马逊待了超过15年，领导过800多人的国际团队。\n我任职期间，亚马逊股票涨幅高达9082%，因此我对普通人的许多生活困境并不了解。比我更高级的副总裁和首席执行官，就更是如此了。谈论自己的财富是大多数高管都避而不谈的禁忌话题。\n今天我想谈谈一个小问题，先介绍一下我自己的情况，就举四点：1) 我没有任何抵押贷款，2) 每两周有一个女佣为我打扫一次住宅，3) 我付钱请别人帮我的花园割草， 4) 我50岁就退休了。\n普通家庭，即使是工程师和知识分子，也没有这些福利。\n在我上面的高管，享有的福利就更多了，我看到的就有：1) 有专门的度假屋，雇了多个工作人员长期看守；2) 私人飞机；3) 私人助理，不用自己付账单、买杂货或接孩子，助理处理一切；4）私人司机；5）孩子上贵得惊人的私立学校；6）他们想住哪儿就住哪儿。\n经济成本从来不是这一类人的障碍。\n现在让我们看一个例子，说明高管与员工是如何脱节的：结束远程工作，重返办公室。\n高管的财富使他们有不同的选择。大多数高管把工作和职业成功放在生活的首位。如果他们不这样做，他们很少能成为高管。他们中的大多数人（包括我自己），利用财富为自己购买时间。他们大部分时间都花在工作上，小部分时间花在家人身上。如果工作是你生活的重心，那么重返办公室就自然是一个优先事项。\n你想象一下高管的工作场景：无需通勤，司机会送你到家，你一心工作，无需赶回家接孩子，助理会帮你做这些，你也无需购物、打扫或做饭，佣人会做这些，无需辅导作业，好学校会提供辅导。在这种情况下，回到办公室感觉非常\"值得\"。\n这不是一篇反对高管个人财富的长篇大论。毕竟，我付出了25年的生命，得到了一些财富。相反，这是一种解释，以便你了解高管与普通员工的脱节。\n如果你需要影响高管，而他们的经历可能与你的现实生活脱节，请通过故事、视频和数据帮助他们看到现实。请记住，他们确实生活在另一个世界。这并不一定会使他们变得邪恶，只是脱节了。我不想发生\"脱节\"，但必须承认这种情况确实会随着时间的推移而发生。\n言论\n1、\n兼职创业不是可以长期坚持的事情。如果你上班时整天面对电脑，回家后又坐在另一台电脑前开发自己的软件，那将让你筋疲力尽。\n你可以这样做几个月，但问题是，企业通常需要更长的时间才能起步，很多人就会放弃。\n-- 《关于独立开发》\n2、\n在开始一个项目时，一定程度的天真是必不可少的。如果我知道这条路有多难，我可能永远不会开始。但由于我完全不了解未来的挑战，所以我只是一头扎进去，一路摸索。 \n-- 金茨·齐巴洛迪斯（Gints Zilbalodis），拉脱维亚导演，他的作品《猫猫的奇幻漂流》（Flow）获得今年的奥斯卡最佳动画片奖\n3、\n你编写的每一行代码都可能是一个潜在的 bug。除非你绝对需要这行代码，缺了它程序就会受影响，否则就不要写。不要编写你用不到的抽象层。如果优化会增加任何复杂性，就坚决不要优化。\n-- 《每一代码都可能是 bug》\n4、\n我对 AI 的看法是，AI 本身不会创造，需要人类与它共同创造，创造的结果好坏与使用它的人的质量高度相关。\n与 AI 交谈不像在与一个人交谈，而像在与人类的集体思维交谈。AI 不应该让你减少思考，而应该帮助你增加思考，AI 是你的杠杆，可以让你拓展自己。\n-- Alex Komoroske，美国程序员\n5、\n10个人开会，可以没有主持人。100个人开会，必须有主持人。1000个人开会，需要一个组委会。\n扩大10倍，需要将知识/资源推向极限，但是扩大100倍，需要跳出现有的维度，重新安排一切。\n-- 《你的下两个零》\n往年回顾\n一周是一年的2%（#293）\n与孔子 AI 聊天（#243）\n前端与后端，谁更难？（#193）\n世界尽头与冷酷仙境（#143）\n（完）\n文档信息\n版权声明：自由转载-非商用-非衍生-保持署名（创意共享3.0许可证）\n发表日期： 2025年3月 7日",
    "link": "http://www.ruanyifeng.com/blog/2025/03/weekly-issue-340.html",
    "pubDate": "2025-03-07T00:11:36.000Z",
    "source": "阮一峰的网络日志",
    "category": "技术博客"
  },
  {
    "id": "rweekly",
    "title": "R Weekly 2025-W12 Multilingual quarto docs, ellmer, cholera in 1854",
    "description": "Hello and welcome to this new issue!\n\r\n\t\t\t\t\nHow to have (my) content shared by R Weekly?\nThis week’s release was curated by Ryo Nakagawara, with help from the R Weekly team members and contributors.\nHighlight\nCreating multilingual documentation with Quarto\nThe ellmer package for using LLMs with R is a game changer for scientists\n{SnowData} 1.0.0: Historical Data from John Snow’s 1854 Cholera Outbreak Map\nInsights\nData Science in the Cloud: A Recap of the Snowflake and Posit Webinar\nEnabling the pharmaceutical programming community to develop ADaM datasets in R: Four Perspectives From the Maintainers\nInitial impressions from testing Posit Connect\nR in Organizations\nR-Ladies is rebranding to R-Ladies+\nR in Academia\nVisualizing Uncertainty (lecture slides)\nResources\nMAGA keyword screening tool: Tool built using Quarto and shinylive that allows you to analyze documents for specific MAGA-targeted keywords in response to this executive order. It is intended to be used to help identify words that might get screened by the federal government, e.g. in a grant proposal for federal funding.\nData Viz Showcase - Paul Schmidt\nResources from NICAR data journalism conferences\nCode Nerd: R Chatgpt helper\nNew Packages\n📦 Go Live for More New Pkgs 📦\n -->\n📦 Keep up to date wtih CRANberries 📦\nCRAN\n{deltatest} 0.1.0: Statistical Hypothesis Testing Using the Delta Method ( cran.r-project.org )\n{SnowData} 1.0.0: Historical Data from John Snow’s 1854 Cholera Outbreak Map\n{RcppDPR} 0.1.9: ‘Rcpp’ Implementation of Dirichlet Process Regression\n{golfr} 0.1.0: Group Assignment Tool\n{ggvfields} 1.0.0: Vector Field Visualizations with ‘ggplot2’\n{sourcoise} 0.5.0: Source a Script and Cache\n{RcensusPkg} 0.1.4: Easily Access US Census Bureau Survey and Geographic Data\n{ambiorix} 2.2.0: Web Framework Inspired by ‘Express.js’\n{cpp11tesseract} 5.3.5: Open Source OCR Engine\n{SeuratExplorer} 0.1.0: An ‘Shiny’ App for Exploring scRNA-seq Data Processed in\n‘Seurat’\n{HARplus} 1.0.1: Enhanced R Package for ‘GEMPACK’ .har and .sl4 Files\n{cpam} 0.1.3: Changepoint Additive Models for Time Series Omics Data\n{CAOP.RAA.2024} 0.0.5: Official Administrative Map of the Azores (CAOP 2024)\n{ravepipeline} 0.0.1: Reproducible Pipeline Infrastructure for Neuroscience\n{VizTest} 0.3: Optimal Confidence Intervals for Visual Testing\n{JMbdirect} 0.1.0: Joint Model for Longitudinal and Multiple Time to Events Data\n{IVDML} 1.0.0: Double Machine Learning with Instrumental Variables and\nHeterogeneity\n{weatherOz} 2.0.0: An API Client for Australian Weather and Climate Data Resources\n{tabtibble} 0.0.1: Simplify Reporting Many Tables\n{clayringsmiletus} 1.0.2: Clay Stacking Rings Found in Miletus (Data)\n{brandr} 0.1.0: Brand Identity Management Using brand.yml Standard\n{geneNR} 1.0.1: Automated Gene Identification for Post-GWAS Analysis\nGitHub or Bitbucket\n{quartabs} 0.1.0.9001: Dynamically generate tabset panels in Quarto HTML documents\n{gsapy} 0.0.0.9000: ‘Shiny’ interface for GSAP JavaScript animations\nUpdated Packages\n🔍 Search on R-universe 🔍\n{httpgd} 2.0.3: A ‘HTTP’ Server Graphics Device - diffify\n{DemographicTable} 0.1.10: Creating Demographic Table - diffify\n{Rforestry} 0.11.1.0: Random Forests, Linear Trees, and Gradient Boosting for\nInference and Interpretability - diffify\n{tzdb} 0.5.0: Time Zone Database Information - diffify\n{greenfeedr} 1.2.0: Process and Report ‘GreenFeed’ Data - diffify\n{ggalign} 1.0.0: A ‘ggplot2’ Extension for Consistent Axis Alignment - diffify\n{accessr} 1.1.2: Command Line Tools to Produce Accessible Documents using ‘R\nMarkdown’ - diffify\n{rjsoncons} 1.3.2: Query, Pivot, Patch, and Validate ‘JSON’ and ‘NDJSON’ - diffify\n{blogdown} 1.21: Create Blogs and Websites with R Markdown - diffify\n{ScottKnott} 1.3-3: The ScottKnott Clustering Algorithm - diffify\n{RCzechia} 1.12.6: Spatial Objects of the Czech Republic - diffify\n{xml2} 1.3.8: Parse XML - diffify\n{ichimoku} 1.5.6: Visualization and Tools for Ichimoku Kinko Hyo Strategies - diffify\n{grateful} 0.2.11: Facilitate Citation of R Packages - diffify\n{mlr3} 0.23.0: Machine Learning in R - Next Generation - diffify\n{gemini.R} 0.9.2: Interface for ‘Google Gemini’ API - diffify\n{wordvector} 0.3.0: Word and Document Vector Models - diffify\n{hexfont} 1.0.0: ‘GNU Unifont’ Hex Fonts - diffify\n{depCensoring} 0.1.7: Statistical Methods for Survival Data with Dependent Censoring - diffify\n{ggeffects} 2.2.1: Create Tidy Data Frames of Marginal Effects for ‘ggplot’ from\nModel Outputs - diffify\n{versioning} 0.2.0: Settings and File I/O using a Configuration YAML File - diffify\n{ggblanket} 12.2.0: Simplify ‘ggplot2’ Visualisation - diffify\n{xmeta} 1.3.3: A Toolbox for Multivariate Meta-Analysis - diffify\n{ggstats} 0.9.0: Extension to ‘ggplot2’ for Plotting Stats - diffify\n{collapse} 2.1.0: Advanced and Fast Data Transformation - diffify\n{politeness} 0.9.4: Detecting Politeness Features in Text - diffify\n{rstan} 2.32.7: R Interface to Stan - diffify\n{uisapi} 0.1.0.9000: Access the UNESCO Institute for Statistics API - diffify\nVideos and Podcasts\nListen to the R-Weekly Highlights Podcast\nShiny Apps\nAppsilon at ShinyConf 2025: Pushing the Boundaries of Shiny Development\nPosit at ShinyConf 2025\nTutorials\nHow to Select Columns from data.table in R\nSparklines in Reactable Tables\nThe ellmer package for using LLMs with R is a game changer for scientists\nMachine Learning Insights on BIST 100’s Future\nCreating data-driven art\n3MW (User-Specific Authentication for Quarto Projects on Azure Static Web Apps)\nDigital Difficulties\nEvidence Synthesis for Decision Making in Healthcare\nConformal prediction intervals with the probably package\nCreating multilingual documentation with Quarto\n\n-->\n\nR Project Updates\nUpdates from R Core:\nCall for Participation\n  \n \nPost by @rOpenSci@hachyderm.io\n View on Mastodon\n  \n\n\nUpcoming Events in 3 Months\nEvents in 3 Months:\nA list of R conferences and meetings\nThis week’s local R-User and applied stats events\nWeekly R Workshops for Ukraine\nShiny in Production 2025: Abstracts Deadline Extension\nEffective Data Visualization in R in Scientific Contexts workshop\nConnect\nJoin the Data Science Learning Community\nrtistry\nMathematical art based on Hilbert curves.  Made with R\n\n#mathart #math #creativecoding #codeart #rtistry\n[image or embed]\n— George Savva (@georgemsavva.bsky.social) March 1, 2025 at 11:03 PM\n\n\nQuotes of the Week\nThe problem with #rstats is portability. Shit like `df$x` has to be converted to `df€x` in Europe and `df£x` in the UK but nobody talks about this.\n— John Coene (@johncoene.bsky.social) March 14, 2025 at 9:42 PM\n\n\n\nI was recently asked to create an Atlas of population diversity for major Spanish municipalities at the census tract level. Who else thinks bivariate maps + scatter plots are a powerhouse combo? Open to suggestions.\n\n#rstats #map #barcelona #diversity\n[image or embed]\n— Juan Galeano (@juangaleano.bsky.social) March 15, 2025 at 11:55 PM\n\n\n\nFeels like air raid alerts in Kyiv are nightly. I checked: since the year’s start, only 13 nights were free of alerts, and each lasts about 131 minutes on average. Let’s see how the ceasefire goes, but meanwhile, please donate to air defense, links in the next reply. #UaView #RStats\n[image or embed]\n— Dariia Mykhailyshyna (@dariia.bsky.social) March 13, 2025 at 12:47 AM",
    "link": "https://rweekly.org//2025-W12.html",
    "pubDate": "Mon, 17 Mar 2025 00:00:00 +0000",
    "source": "R Weekly",
    "category": "R语言"
  },
  {
    "id": "rweekly",
    "title": "R Weekly 2025-W11 LaTeX Typesetting in R, Create a unique documentation for your R Package",
    "description": "Hello and welcome to this new issue!\n\r\n\t\t\t\t\nHow to have (my) content shared by R Weekly?\nThis week’s release was curated by Batool Almaarzouq, with help from the R Weekly team members and contributors.\nHighlight\nLaTeX Typesetting in R: The ‘xdvir’ Package\nCustomize your expedition: Create a unique documentation for your R Package\nInsights\nCRAN-like repository for most recent releases of Techtonique’s R packages\nAdding arm64 to r2u\nBest Before Dates by Bass\n\nResources\n2025-01  LaTeX Typesetting in R\nggplot2: Going further in the tidyverse\n\nWhat They Forgot to Teach You About R\n9 new books added to Big Book of R\nWbsite for tidyplots use cases\n\nNew Packages\n📦 Go Live for More New Pkgs 📦\n -->\n📦 Keep up to date wtih CRANberries 📦\nCRAN\n{forgts} 0.0.1: Reads a spreadsheet and its formatting information to produce gt tables with the same cell and text formatting as the input file\n{Certara.DarwinReporter} 2.0.1: Data Visualization Utilities for ‘pyDarwin’ Machine Learning\nPharmacometric Model Development\n{xlr} 1.0.3: Create Table Summaries and Export Neat Tables to ‘Excel’\n{visae} 0.2.1: Visualization of Adverse Events\n{shinyTimer} 0.1.0: Customizable Timer for ‘shiny’ Applications\n{uisapi} 0.1.0: Access the UNESCO Institute for Statistics API\n{RplotterPkg} 0.1.3: R Plotting Functions Using ‘ggplot2’\n{revise} 0.1.0: Dynamic Revision Letters for ‘Rmarkdown’ Manuscripts\n{connected} 1.1: Visualize and Improve Connectedness of Factors in Tables\n{mixtree} 0.0.1: A Statistical Framework for Comparing Sets of Trees\n{MHQoL} 0.12.0: Mental Health Quality of Life Toolkit\n{rconf} 0.1.2: Minimal and Lightweight Configuration Tool\n{mbX} 0.1.3: A Comprehensive Microbiome Data Processing Pipeline\n{IndexNumberTools} 1.1: Working with Index Numbers\n{sakura} 0.1.0: Extension to R Serialization\n{rmsMD} 0.1.2: Output Results from ‘rms’ Models for Medical Journals\n{maths.genealogy} 0.1.2: Mathematics PhD Genealogy Data and Plotting\n{Certara.ModelResults} 3.0.1: Generate Diagnostics for Pharmacometric Models Using ‘shiny’\nUpdated Packages\n🔍 Search on R-universe 🔍\nRcppSimdJson 0.1.13 on CRAN: Compiler Nag, New Upsteam\nRcppDate 0.0.5: Address Minor Compiler Nag\n{mrbin} 1.9.0: Metabolomics Data Analysis Functions - diffify\n{tidyllm} 0.3.2: Tidy Integration of Large Language Models - diffify\n{tidyplots} 0.2.2: Tidy Plots for Scientific Papers - diffify\n\n{gimap} 1.0.3: Calculate Genetic Interactions for Paired CRISPR Targets - diffify\n{pals} 1.10: Color Palettes, Colormaps, and Tools to Evaluate Them - diffify\n{readxl} 1.4.5: Read Excel Files - diffify\n{archeofrag} 1.1.0: Spatial Analysis in Archaeology from Refitting Fragments - diffify\n{PubChemR} 2.1.4: Interface to the ‘PubChem’ Database for Chemical Data Retrieval - diffify\n{rgeoda} 0.1.0: R Library for Spatial Data Analysis - diffify\n{rhub} 2.0.1: Tools for R Package Developers - diffify\n{usmapdata} 0.4.0: Mapping Data for ‘usmap’ Package - diffify\n{hutilscpp} 0.10.8: Miscellaneous Functions in C++ - diffify\n{habtools} 1.1.1: Tools and Metrics for 3D Surfaces and Objects - diffify\n{rnpn} 1.3.0: Interface to the National ‘Phenology’ Network ‘API’ - diffify\n{divest} 1.2.0: Get Images Out of DICOM Format Quickly - diffify\n{dataquieR} 2.5.1: Data Quality in Epidemiological Research - diffify\n{tutorial.helpers} 0.4.2: Helper Functions for Creating Tutorials - diffify\n{strata} 1.4.3: Simple Framework for Simple Automation - diffify\n{leaflet.extras2} 1.3.1: Extra Functionality for ‘leaflet’ Package - diffify\n{huxtable} 5.6.0: Easily Create and Style Tables for LaTeX, HTML and Other Formats - diffify\n{scoutbaR} 0.1.0: A Spotlight ‘React’ Widget for ‘shiny’ Apps - diffify\n{isopleuros} 1.4.0: Ternary Plots - diffify\n{formatBibtex} 0.1.1: Format BibTeX Entries and Files - diffify\n{xpectr} 0.4.4: Generates Expectations for ‘testthat’ Unit Testing - diffify\n{offsetreg} 1.1.1: An Extension of ‘Tidymodels’ Supporting Offset Terms - diffify\nVideos and Podcasts\nListen to the R-Weekly Highlights Podcast\nUse Claude Code in RStudio to ship R code like some kind of 10x tidyverse developer\nR for Data Science: Web scraping\nR Internationally\nUnir y combinar gráficos {ggplot2} en R\nTutorials\nA Bayesian proportional hazards model with a penalized spline\nCustomize your expedition: Create a unique documentation for your R Package\n\nThe Complete Guide to Handling NA Values in R Tables: Methods, Best Practices, and Solutions\nHarness Local LLMs and GitHub Copilot for Enhanced R Package Development\n3MW (GitHub Authentication for Azure Static Web Apps)\nHow to include control variables in a cross-lagged panel model\n\n-->\n\nR Project Updates\nUpdates from R Core:\nUpcoming Events in 3 Months\nEvents in 3 Months:\nA list of R conferences and meetings\nThis week’s local R-User and applied stats events\nRegister for R/Pharma at posit::conf(2025)\nWeekly R Workshops for Ukraine\nGrants & Funding\n2025 ISC Grant Program\nConnect\nJoin the Data Science Learning Community\nrtistry\nCrochet-inspired generative art 🧶🎨\n\n#Rtistry #RStats #ggplot2 #GenerativeArt\n[image or embed]\n— Nicola Rennie (@nrennie.bsky.social) March 7, 2025 at 4:23 PM\n\n\nQuotes of the Week\n  \n \nPost by @rladies_paris@mastodon.social\n View on Mastodon\n  \n\n\n\nReceiving CRAN feedback\n\n#RStats\n[image or embed]\n— coolbutuseless (@coolbutuseless.fosstodon.org.ap.brid.gy) March 8, 2025 at 3:16 PM",
    "link": "https://rweekly.org//2025-W11.html",
    "pubDate": "Mon, 10 Mar 2025 00:00:00 +0000",
    "source": "R Weekly",
    "category": "R语言"
  },
  {
    "id": "rweekly",
    "title": "R Weekly 2025-W10 ellmer, Closeread Prize winnners, Rapid RAG Prototyping",
    "description": "Hello and welcome to this new issue!\n\r\n\t\t\t\t\nHow to have (my) content shared by R Weekly?\nThis week’s release was curated by Sam Parmar, with help from the R Weekly team members and contributors.\nHighlight\nAnnouncing ellmer: A package for interacting with Large Language Models in R\nWinners of the Closeread Prize – Data-Driven Scrollytelling with Quarto\nRapid RAG Prototyping: Building a Retrieval Augmented Generation Prototype with ellmer and DuckDB\nInsights\nAnnouncing ellmer: A package for interacting with Large Language Models in R\n\nLLM + Quarto: Turn One-Off Reports Into Automated Solutions\nShare your data apps and docs more seamlessly on Connect Cloud\nJanuary 2025 Top 40 New CRAN Packages\nQ1 2025 tidymodels digest\n\nUnderstand geom_bar and its statistical transformations\nAnalyzing targeted locus amplification (TLA) data\nWinners of the Closeread Prize – Data-Driven Scrollytelling with Quarto\n\nR in the Browser: Announcing Our WebAssembly Distribution\nR in the Real World\nPharmaverse Council Member updates\nR Submissions Working Group: Pilot 5 Launch and more!\nR in Organizations\nWorking with Clinical Trial Data? There’s a Pharmaverse Package for That\nResources\nCRANhaven - A repository for recently archived CRAN packages\nNew Packages\n📦 Go Live for More New Pkgs 📦\n -->\n📦 Keep up to date wtih CRANberries 📦\nCRAN\n{httpgd} 2.0.3: A ‘HTTP’ Server Graphics Device\n{awdb} 0.1.1: Query the USDA NWCC Air and Water Database REST API\n{cnd} 0.1.0: Create and Register Conditions\n{EpiSimR} 1.1: A ‘Shiny’ App to Simulate the Dynamics of Epidemic and Endemic Diseases Spread\n{xdvir} 0.1-2: Render ‘LaTeX’ in Plots\n‘shiny’\n{crane} 0.1.0: Supplements the ‘gtsummary’ Package for Pharmaceutical Reporting\nUpdated Packages\n🔍 Search on R-universe 🔍\n{cards} 0.5.1: Analysis Results Data - diffify\n{pander} 0.6.6: An R ‘Pandoc’ Writer - diffify\n{jobqueue} 1.5.1: Run Interruptible Code Asynchronously - diffify\n{xml2} 1.3.7: Parse XML - diffify\n{teal.modules.general} 0.4.0: General Modules for ‘teal’ Applications - diffify\n{teal.modules.clinical} 0.10.0: ‘teal’ Modules for Standard Clinical Outputs - diffify\n{rayrender} 0.38.10: Build and Raytrace 3D Scenes - diffify\n{r2rtf} 1.1.3: Easily Create Production-Ready Rich Text Format (RTF) Tables and\nFigures - diffify\n{odbc} 1.6.0: Connect to ODBC Compatible Databases (using the DBI Interface) - diffify\n{diseasystore} 0.3.1: Feature Stores for the ‘diseasy’ Framework - diffify\n{CNID} 2.0.2: Get Basic Information from Chinese ID Number - diffify\n{eq5d} 0.15.7: Methods for Analysing ‘EQ-5D’ Data and Calculating ‘EQ-5D’ Index\nScores - diffify\n{splines2} 0.5.4: Regression Spline Functions and Classes - diffify\n{pmlbr} 0.3.0: Interface to the Penn Machine Learning Benchmarks Data\nRepository - diffify\n{healthdb} 0.4.0: Working with Healthcare Databases - diffify\n{censobr} 0.4.1: Download Data from Brazil’s Population Census - diffify\n{xslt} 1.5.1: Extensible Style-Sheet Language Transformations - diffify\n{readxl} 1.4.4: Read Excel Files - diffify\n{duckplyr} 1.0.1: A ‘DuckDB’-Backed Version of ‘dplyr’ - diffify\n{Rfast} 2.1.5: A Collection of Efficient and Extremely Fast R Functions - diffify\n{litedown} 0.6: A Lightweight Version of R Markdown - diffify\n{gtreg} 0.4.1: Regulatory Tables for Clinical Research - diffify\n{aplot} 0.2.5: Decorate a ‘ggplot’ with Associated Information - diffify\n{sasr} 0.1.4: ‘SAS’ Interface - diffify\n{tinytex} 0.56: Helper Functions to Install and Maintain TeX Live, and Compile LaTeX Documents - diffify\n{QuickJSR} 1.6.0: Interface for the ‘QuickJS’ Lightweight ‘JavaScript’ Engine - diffify\n{PatientProfiles} 1.3.0: Identify Characteristics of Patients in the OMOP Common Data Model - diffify\n{reproducibleRchunks} 1.0.3: Automated Reproducibility Checks for R Markdown Documents - diffify\n{simer} 1.0.0: Data Simulation for Life Science and Breeding - diffify\n{summarytools} 1.1.1: Tools to Quickly and Neatly Summarize Data - diffify\n{flexlsx} 0.3.4: Exporting ‘flextable’ to ‘xlsx’ Files - diffify\n{zippeR} 0.1.1: Working with United States ZIP Code and ZIP Code Tabulation Area\nData - diffify\n{and} 0.1.6: Construct Natural-Language Lists with Internationalization - diffify\n{dittoViz} 1.0.3: User Friendly Data Visualization - diffify\n{brickset} 2025.0.0: Interface with the Brickset API for Getting Data About LEGO Sets - diffify\n{khroma} 1.16.0: Colour Schemes for Scientific Data Visualization - diffify\n{arkhe} 1.10.0: Tools for Cleaning Rectangular Data - diffify\n{akc} 0.9.9.1: Automatic Knowledge Classification - diffify\n{watcher} 0.1.2: Watch the File System for Changes - diffify\n{RPostgres} 1.4.8: C++ Interface to PostgreSQL - diffify\n{networktools} 1.6.0: Tools for Identifying Important Nodes in Networks - diffify\n{R.utils} 2.13.0: Various Programming Utilities - diffify\n{reticulate} 1.41.0: Interface to ‘Python’ - diffify\n{ggnewscale} 0.5.1: Multiple Fill and Colour Scales in ‘ggplot2’ - diffify\n{teal} 0.16.0: Exploratory Web Apps for Analyzing Clinical Trials Data - diffify\n{simDAG} 0.2.2: Simulate Data from a DAG and Associated Node Information - diffify\n{planr} 0.5.1: Tools for Supply Chain Management, Demand and Supply Planning - diffify\n{plotscaper} 0.2.8: Explore Your Data with Interactive Figures - diffify\nVideos and Podcasts\nListen to the R-Weekly Highlights Podcast\nR-Ladies Rome (English) - Interactive R, Python, and Shiny in the Browser with Quarto and Shinylive\nMaster Data Extraction - Turn Texts into Tidy Data with R & {ellmer}\nCompany-branded reports, apps, and dashboards made easier with brand.yml & Posit\nTutorials\nChecking your R packages and practicals on a schedule using GitHub Actions\n3MW (Authentication for Quarto Projects on Azure)\nDependency-light hex stickers with {gex}\nWeb app with DeepSeek R1 and Hugging Face API for chatting\nHow to Create Tables in R (With Examples) – A Comprehensive Guide Using Base R, dplyr, and data.table\nRapid RAG Prototyping: Building a Retrieval Augmented Generation Prototype with ellmer and DuckDB\n\n-->\n\nR Project Updates\nUpdates from R Core:\nCall for Participation\nUpcoming Events in 3 Months\nEvents in 3 Months:\nA list of R conferences and meetings\nThis week’s local R-User and applied stats events\nWeekly R Workshops for Ukraine\nFrame-by-Frame Modeling and Validation of NFL geospatial data using gganimate in R workshop\nIntroduction to Empirical Macroeconomics with R workshop\nRix: reproducible data science environments with Nix\nGrants & Funding\n2025 ISC Grant Program\nConnect\nJoin the Data Science Learning Community\nrtistry\n  \n \nPost by @nrennie@fosstodon.org\n View on Mastodon\n  \n\n\nQuotes of the Week\n#TodayinHistory #dataviz #Onthisday #OTD 📊\n🎂Happy #Rstats birthday!\nR 1.0.0 was first released on February 29, 2000.\nDoes that make it 24 or just 6 leap-years old? pic.twitter.com/KpZordLXAt\n— Michael Friendly @datavisfriendly.bsky.social (@datavisFriendly) March 1, 2025",
    "link": "https://rweekly.org//2025-W10.html",
    "pubDate": "Mon, 03 Mar 2025 00:00:00 +0000",
    "source": "R Weekly",
    "category": "R语言"
  },
  {
    "id": "rweekly",
    "title": "R Weekly 2025-W09 nhyris, tisthemachinelearner, Forks",
    "description": "Hello and welcome to this new issue!\n\r\n\t\t\t\t\nHow to have (my) content shared by R Weekly?\nThis week’s release was curated by Colin Fay, with help from the R Weekly team members and contributors.\nHighlight\nnhyris - The minimal framework for transform R shiny application into standalone\ntisthemachinelearner: A Lightweight interface to scikit-learn with 2 classes, Classifier and Regressor (in Python and R)\nThe Dynamic Relationship of Forks with their Upstream Repository\nInsights\nWhy we forked nixpkgs\nRcpp now used by 3000 CRAN packages!\nThe Dynamic Relationship of Forks with their Upstream Repository\nR in Academia\nWin a Battle in the Game of Risk\nResources\nnhyris - The minimal framework for transform R shiny application into standalone\nNew Packages\n📦 Go Live for More New Pkgs 📦\n -->\n📦 Keep up to date wtih CRANberries 📦\nCRAN\n{GitAI} 0.1.0: Extracts Knowledge from ‘Git’ Repositories\n{codelist} 0.1.0: Working with Code Lists\n{netknitr} 0.2.1: Knit Network Map for any Dataset\n{aftables} 1.0.2: Create Spreadsheet Publications Following Best Practice\n{musicXML} 1.0.1: Data Sonification using ‘musicXML’\nUpdated Packages\n🔍 Search on R-universe 🔍\nRcppDE 0.1.8 on CRAN: Maintenance\n{summarytools} 1.1.0: Tools to Quickly and Neatly Summarize Data - diffify\n{duckdb} 1.2.0: DBI Package for the DuckDB Database Management System - diffify\n{cpp11armadillo} 0.4.4: An ‘Armadillo’ Interface - diffify\n{parseLatex} 0.3.0: Parse ‘LaTeX’ Code - diffify\n{flint} 0.0.2: Fast Library for Number Theory - diffify\nVideos and Podcasts\nListen to the R-Weekly Highlights Podcast\nCollaborating Across Pharma: Open Source Highlights from the PHUSE US Connect 2024 Keynote\nTutorials\ntisthemachinelearner: A Lightweight interface to scikit-learn with 2 classes, Classifier and Regressor (in Python and R)\nBurn Notice\nHappy Valentine’s Day\nHow to Replace Values in Data Frame Based on Lookup Table in R\n3MW (Host Quarto Projects on Azure)\n5 Levels of Data Wrangling Every R User Must Master\nCreating a Finder Smart Folder of your RStudio Project files to enable super fast project launching\nCreating R, Python, Stata, and Julia tutorial worksheets (with and without solutions) using Quarto\nCreating effectively multi-engine Quarto documents using Quarto’s embed shortcode\nA First Look at TimeGPT using nixtlar\nHow to use a histogram as a legend in {ggplot2}\n\n-->\n\nR Project Updates\nUpdates from R Core:\nCall for Participation\nShiny in Production 2025: Call for Abstracts\nGovernment Advances in Statistical Programming (GASP) Conference 2025: Call for Abstracts\nUpcoming Events in 3 Months\nEvents in 3 Months:\nA list of R conferences and meetings\nThis week’s local R-User and applied stats events\nWeekly R Workshops for Ukraine\nGrants & Funding\nDiscover posit::conf as an Opportunity Scholar\nConnect\nJoin the Data Science Learning Community",
    "link": "https://rweekly.org//2025-W09.html",
    "pubDate": "Mon, 24 Feb 2025 00:00:00 +0000",
    "source": "R Weekly",
    "category": "R语言"
  },
  {
    "id": "tidyverse",
    "title": "Improved sparsity support in tidymodels",
    "description": "Photo by Oliver Olah on Unsplash\n5x height\n* [x] [`hugodown::use_tidy_thumbnails()`](https://rdrr.io/pkg/hugodown/man/use_tidy_post.html)\n* [x] Add intro sentence, e.g. the standard tagline for the package\n* [x] [`usethis::use_tidy_thanks()`](https://usethis.r-lib.org/reference/use_tidy_thanks.html)\n-->\nWe’re stoked to announce tidymodels now fully supports sparse data from end to end. We have been working on this for \nover 5 years. This is an extension of the work we have done \npreviously with blueprints, which would carry the data sparsely some of the way.\nYou will need \nrecipes 1.2.0, \nparsnip 1.3.0, \nworkflows 1.2.0 or later for this to work.\nWhat are sparse data?\n  \n    \n      \n\n      \n\n    \n  \n\nThe term sparse data refers to a data set containing many zeroes. Sparse data appears in all kinds of fields and can be produced in a number of preprocessing methods. The reason why we care about sparse data is because of how computers store numbers. A 32-bit integer value takes 4 bytes to store. An array of 32-bit integers takes 40 bytes, and so on. This happens because each value is written down.\nA sparse representation instead stores the locations and values of the non-zero entries. Suppose we have the following vector with 20 entries:\nc(0, 0, 1, 0, 3, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)\n\n\nIt could be represented sparsely using the 3 values positions = c(1, 3, 7), values = c(3, 5, 8), and length = 20. Now, we have seven values to represent a vector of 20 elements. Since some modeling tasks contain even sparser data, this type of representation starts to show real benefits in terms of execution time and memory consumption.\nThe tidymodels set of packages has undergone several internal changes to allow it to represent data sparsely internally when it would be beneficial. These changes allow you to fit models that contain sparse data faster and more memory efficiently than before. Moreover, it allows you to fit models previously not possible due to them not fitting in memory.\nSparse matrix support\n  \n    \n      \n\n      \n\n    \n  \n\nThe first benefit of these changes is that recipe(), prep(), bake(), fit(), and \npredict() now accept sparse matrices created using the Matrix package.\nThe permeability_qsar data set from the modeldata package contains quite a lot of zeroes in the predictors, so we will use it as a demonstration. Starting by coercing it into a sparse matrix.\nlibrary(tidymodels)\nlibrary(Matrix)\npermeability_sparse <- as(as.matrix(permeability_qsar), \"sparseMatrix\")\nWe can now use this sparse matrix in our code the same way as a dense matrix or data frame:\nrec_spec <- recipe(permeability ~ ., data = permeability_sparse) |>\n  step_zv(all_predictors())\n\nmod_spec <- boost_tree(\"regression\", \"xgboost\")\n\nwf_spec <- workflow(rec_spec, mod_spec)\nModel training has the usual syntax:\nwf_fit <- fit(wf_spec, permeability_sparse)\nas does prediction:\npredict(wf_fit, permeability_sparse)\n#> # A tibble: 165 × 1\n#>     .pred\n#>     <dbl>\n#>  1 10.5  \n#>  2  1.50 \n#>  3 13.1  \n#>  4  1.10 \n#>  5  1.25 \n#>  6  0.738\n#>  7 29.3  \n#>  8  2.44 \n#>  9 36.3  \n#> 10  4.31 \n#> # ℹ 155 more rows\n\nNote that only some models/engines work well with sparse data. These are all listed here https://www.tidymodels.org/find/sparse/. If the model doesn’t support sparse data, it will be coerced into the default non-sparse representation and used as usual.\nWith a few exceptions, it should work like any other data set. However, this approach has two main limitations. The first is that we are limited to regression tasks since the outcome has to be numeric to be part of the sparse matrix.\nThe second limitation is that it only works with non-formula methods for parsnip and workflows. This means that you can use a recipe with add_recipe() or select variables directly with add_variables() when using a workflow. And you need to use fit_xy() instead of fit() when using a parsnip object by itself.\nIf this is of interest we also have a https://www.tidymodels.org/ post about \nusing sparse matrices in tidymodels.\nSparse data from recipes steps\n  \n    \n      \n\n      \n\n    \n  \n\nWhere this sparsity support really starts to shine is when the recipe we use will generate sparse data. They come in two flavors, sparsity creation steps and sparsity preserving steps. Both listed here: https://www.tidymodels.org/find/sparse/.\nSome steps like step_dummy(), step_indicate_na(), and \ntextrecipes::step_tf() will almost always produce a lot of zeroes. We take advantage of that by generating it sparsely when it is beneficial. If these steps end up producing sparse vectors, we want to make sure the sparsity is preserved. A couple of handfuls of steps, such as step_impute_mean() and step_scale(), have been updated to be able to work efficiently with sparse vectors. Both types of steps are detailed in the above-linked list of compatible methods.\nWhat this means in practice is that if you use a model/engine that supports sparse data and have a recipe that produces enough sparse data, then the steps will switch to produce sparse data by using a new sparse data format to store the data (when appropriate) as the recipe is being processed. Then if the model can accept sparse objects, we convert the data from our new sparse format to a standard sparse matrix object. Increasing performance when possible while preserving performance otherwise.\nBelow is a simple recipe using the ames data set. step_dummy() is applied to all the categorical predictors, leading to a significant amount of zeroes.\nrec_spec <- recipe(Sale_Price ~ ., data = ames) |>\n  step_zv(all_predictors()) |>\n  step_normalize(all_numeric_predictors()) |>\n  step_dummy(all_nominal_predictors())\n\nmod_spec <- boost_tree(\"regression\", \"xgboost\")\n\nwf_spec <- workflow(rec_spec, mod_spec)\nWhen we go to fit it now, it takes around 125ms and allocates 37.2MB. Compared to before these changes it would take around 335ms and allocate 67.5MB.\nwf_fit <- fit(wf_spec, ames)\nWe see similar speedups when we predictor with around 20ms and 25.2MB now, compared to around 60ms and 55.6MB before.\npredict(wf_fit, ames)\n#> # A tibble: 2,930 × 1\n#>      .pred\n#>      <dbl>\n#>  1 208649.\n#>  2 115339.\n#>  3 148634.\n#>  4 239770.\n#>  5 190082.\n#>  6 184604.\n#>  7 208572.\n#>  8 177403 \n#>  9 261000.\n#> 10 198604.\n#> # ℹ 2,920 more rows\n\nThese improvements are tightly related to memory allocation, which depends on the sparsity of the data set produced by the recipe. This is why it is hard to say how much benefit you will see. We have seen orders of magnitudes of improvements, both in terms of time and memory allocation. We have also been able to fit models where previously the data was too big to fit in memory.\nPlease see the post on tidymodels.org, which goes into more detail about when you are likely to benefit from this and how to change your recipes and workflows to take full advantage of this new feature.\nThere is also a https://www.tidymodels.org/ post going into a bit more detail about how to \nuse recipes to produce sparse data.",
    "link": "https://www.tidyverse.org/blog/2025/03/tidymodels-sparsity/",
    "pubDate": "Wed, 19 Mar 2025 00:00:00 +0000",
    "source": "Tidyverse Blog",
    "category": "R语言"
  },
  {
    "id": "tidyverse",
    "title": "Q1 2025 tidymodels digest",
    "description": "5x height\n* [ ] `hugodown::use_tidy_thumbnails()`\n* [ ] Add intro sentence, e.g. the standard tagline for the package\n* [ ] `usethis::use_tidy_thanks()`\n-->\nThe tidymodels framework is a collection of R packages for modeling and machine learning using tidyverse principles.\nSince the beginning of 2021, we have been publishing quarterly updates here on the tidyverse blog summarizing what’s new in the tidymodels ecosystem. The purpose of these regular posts is to share useful new features and any updates you may have missed. You can check out the tidymodels tag to find all tidymodels blog posts here, including our roundup posts as well as those that are more focused.\nWe’ve sent a steady stream of tidymodels packages to CRAN recently. We usually release them in batches since many of our packages are tightly coupled with one another. Internally, this process is referred to as the “cascade” of CRAN submissions.\nThe post will update you on which packages have changed and the major improvements you should know about.\nHere’s a list of the packages and their News sections:\nbaguette\nbrulee\ncensored\ndials\nhardhat\nparsnip\nrecipes\ntidymodels\ntune\nworkflows\nLet’s look at a few specific updates.\nImprovements in errors and warnings\n  \n    \n      \n\n      \n\n    \n  \n\nA group effort was made to improve our error and warning messages across many packages. This started with an internal “upkeep week” (which ended up being 3-4 weeks) and concluded at the \nTidy Dev Day in Seattle after posit::conf(2024).\nThe goal was to use new tools in the cli and rlang packages to make messages more informative than they used to be. For example, using:\ntidy(pca_extract_trained, number = 3, type = \"variances\")\n\n\nused to result in the error message:\nError in `match.arg()`:\n! 'arg' should be one of \"coef\", \"variance\"\n\nThe new system references the function that you called and not the underlying base R function that actually errored. It also suggests a solution:\nError in `tidy()`:\n! `type` must be one of \"coef\" or \"variance\", not \"variances\".\ni Did you mean \"variance\"?\n\nThe rlang package created a set of \nstandalone files that contain high-quality type checkers and related functions. This also improves the information that users get from an error. For example, using an inappropriate formula value in fit(linear_reg(), \"boop\", mtcars), the old message was:\nError in `fit()`:\n! The `formula` argument must be a formula, but it is a <character>.\n\nand now you see:\nError in `fit()`:\n! `formula` must be a formula, not the string \"boop\".\n\nThis was a lot of work and we’re still aren’t finished. Two events helped us get as far as we did.\nFirst, Simon Couch made the \nchores package (its previous name was “pal”), which enabled us to use AI tools to solve small-scope problems, such as converting old rlang error code to use the new \ncli syntax. I can’t overstate how much of a speed-up this was for us.\nSecond, at developer day, many external folks pitched in to make pull requests from a list of issues:\nOrganizing Tidy Dev Day issues.\nI love these sessions for many reasons, but mostly because we meet users and contributors to our packages in person and work with them on specific tasks.\nThere is a lot more to do here; we have a lot of secondary packages that would benefit from these improvements too.\nQuantile regression in parsnip\n  \n    \n      \n\n      \n\n    \n  \n\nOne big update in parsnip was a new modeling mode of \"quantile regression\". Daniel McDonald and Ryan Tibshirani largely provided some inertia for this work based on their \ndisease modeling framework.\nYou can generate quantile predictions by first creating a model specification, which includes the quantiles that you want to predict:\nlibrary(tidymodels)\ntidymodels_prefer()\n\names <- \n  modeldata::ames |> \n  mutate(Sale_Price = log10(Sale_Price)) |> \n  select(Sale_Price, Latitude)\n\nquant_spec <- \n  linear_reg() |> \n  set_engine(\"quantreg\") |> \n  set_mode(\"quantile regression\", quantile_levels = c(0.1, 0.5, 0.9))\nquant_spec\n\n\n## Linear Regression Model Specification (quantile regression)\n## \n## Computational engine: quantreg\n\n## Quantile levels: 0.1, 0.5, and 0.9.\n\nWe’ll add some spline terms via a recipe and fit the model:\nspline_rec <- \n  recipe(Sale_Price ~ ., data = ames) |> \n  step_spline_natural(Latitude, deg_free = 10)\n\nquant_fit <- \n  workflow(spline_rec, quant_spec) |> \n  fit(data = ames)\n\nquant_fit\n\n\n## ══ Workflow [trained] ═════════════════════════════════════════════════\n## Preprocessor: Recipe\n## Model: linear_reg()\n## \n## ── Preprocessor ───────────────────────────────────────────────────────\n## 1 Recipe Step\n## \n## • step_spline_natural()\n## \n## ── Model ──────────────────────────────────────────────────────────────\n## Call:\n## quantreg::rq(formula = ..y ~ ., tau = quantile_levels, data = data)\n## \n## Coefficients:\n##               tau= 0.1    tau= 0.5    tau= 0.9\n## (Intercept) 4.71981123  5.07728741  5.25221335\n## Latitude_01 1.22409173  0.70928577  0.79000849\n## Latitude_02 0.19561816  0.04937750  0.02832633\n## Latitude_03 0.16616065  0.02045910  0.14730573\n## Latitude_04 0.30583648  0.08489487  0.15595080\n## Latitude_05 0.21663212  0.02016258 -0.01110625\n## Latitude_06 0.33541228  0.12005254  0.03006777\n## Latitude_07 0.47732205  0.09146728  0.17394021\n## Latitude_08 0.24028784  0.30450058  0.26144584\n## Latitude_09 0.05840312 -0.14733781 -0.11911843\n## Latitude_10 1.52800673  0.95994216  1.21750501\n## \n## Degrees of freedom: 2930 total; 2919 residual\n\nFor prediction, tidymodels always returns a data frame with as many rows as the input data set (here: ames). The result for quantile predictions is a special vctrs class:\nquant_pred <- predict(quant_fit, ames) \nquant_pred |> slice(1:4)\n\n\n## # A tibble: 4 × 1\n##   .pred_quantile\n##        <qtls(3)>\n## 1         [5.33]\n## 2         [5.33]\n## 3         [5.33]\n## 4         [5.31]\n\n\nclass(quant_pred$.pred_quantile)\n\n\n## [1] \"quantile_pred\" \"vctrs_vctr\"    \"list\"\n\nwhere the output [5.31] shows the middle quantile.\nWe can expand the set of quantile predictions so that there are three rows for each source row in ames. There’s also an integer column called .row so that we can merge the data with the source data:\nquant_pred$.pred_quantile[1]\n\n\n## <quantiles[1]>\n## [1] [5.33]\n## # Quantile levels: 0.1 0.5 0.9\n\n\nas_tibble(quant_pred$.pred_quantile[1])\n\n\n## # A tibble: 3 × 3\n##   .pred_quantile .quantile_levels  .row\n##            <dbl>            <dbl> <int>\n## 1           5.08              0.1     1\n## 2           5.33              0.5     1\n## 3           5.52              0.9     1\n\nHere are the predicted quantile values:\nquant_pred$.pred_quantile |> \n  as_tibble() |> \n  full_join(ames |> add_rowindex(), by = \".row\") |> \n  arrange(Latitude) |> \n  ggplot(aes(x = Latitude)) + \n  geom_point(data = ames, aes(y = Sale_Price), alpha = 1 / 5) +\n  geom_line(aes(y = .pred_quantile, col = format(.quantile_levels)), \n            show.legend = FALSE, linewidth = 1.5) \n\n\n10%, 50%, and 90% quantile predictions.\nFor now, the new mode does not have many engines. We need to implement some performance statistics in the yardstick package before integrating these models into the whole tidymodels ecosystem.\nIn other news, we’ve added some additional neural network models based on some improvements in the brulee package. Namely, two-layer networks can be tuned for feed-forward networks on tabular data (using torch).\nOne other improvement has been simmering for a long time: the ability to exploit sparse data structures better. We’ve improved our fit() interfaces for the few model engines that can use sparsely encoded data. There is much more to come on this in a few months, especially around recipes, so stay tuned.\nFinally, we’ve created a set of \nchecklists that can be used when creating new models or engines. These are very helpful, even for us, since there is a lot of minutiae to remember.\nParallelism in tune\n  \n    \n      \n\n      \n\n    \n  \n\nThis was a small maintenance release mostly related to parallel processing. Up to now, tune facilitated parallelism using the \nforeach package. That package is mature but not actively developed, so we have been slowly moving toward using the \nfuture package(s).\nThe \nfirst step in this journey was to keep using foreach internally (but lean toward future) but to encourage users to move from directly invoking the foreach package and, instead, load and use the future package.\nWe’re now moving folks into the second stage. tune will now raise a warning when:\nA parallel backend has been registered with foreach, and\nNo \nplan() has been specified with future.\nThis will allow users to transition their existing code to only future and allow us to update existing documentation and training materials.\nWe anticipate that the third stage, removing foreach entirely, will occur sometime before posit::conf(2025) in September.\nThings to look forward to\n  \n    \n      \n\n      \n\n    \n  \n\nWe are working hard on a few major initiatives that we plan on showing off at \nposit::conf(2025).\nFirst is integrated support for sparse data. The emphasis is on “data” because users can use a data frame of sparse vectors or the usual sparse matrix format. This is a big deal because it does not force you to convert non-numeric data into a numeric matrix format. Again, we’ll discuss this more in the future, but you should be able to use sparse data frames in parsnip, recipes, tune, etc.\nThe second initiative is the longstanding goal of adding postprocessing to tidymodels. Just as you can add a preprocessor to a model workflow, you will be able to add a set of postprocessing adjustments to the predictions your model generates. See our \nprevious post for a sneak peek.\nFinally, this year’s \nsummer internship focuses on supervised feature selection methods. We’ll also have releases (and probably another package) for these tools.\nThese should come to fruition (and CRAN) before or around August 2025.\nAcknowledgements\n  \n    \n      \n\n      \n\n    \n  \n\nWe want to sincerely thank everyone who contributed to these packages since their previous versions:\n@AlbertoImg, \n@asb2111, \n@balraadjsings, \n@bcjaeger, \n@beansrowning, \n@BrennanAntone, \n@cheryldietrich, \n@chillerb, \n@conarr5, \n@corybrunson, \n@dajmcdon, \n@davidrsch, \n@Edgar-Zamora, \n@EmilHvitfeldt, \n@gaborcsardi, \n@gimholte, \n@grantmcdermott, \n@grouptheory, \n@hfrick, \n@ilaria-kode, \n@JamesHWade, \n@jesusherranz, \n@jkylearmstrong, \n@joranE, \n@joscani, \n@Joscelinrocha, \n@josho88, \n@joshuagi, \n@JosiahParry, \n@jrosell, \n@jrwinget, \n@KarlKoe, \n@kscott-1, \n@lilykoff, \n@lionel-, \n@LouisMPenrod, \n@luisDVA, \n@marcelglueck, \n@marcozanotti, \n@martaalcalde, \n@mattwarkentin, \n@mihem, \n@mitchellmanware, \n@naokiohno, \n@nhward, \n@npelikan, \n@obgeneralao, \n@owenjonesuob, \n@pbhogale, \n@Peter4801, \n@pgg1309, \n@reisner, \n@rfsaldanha, \n@rkb965, \n@RobLBaker, \n@RodDalBen, \n@SantiagoD999, \n@shum461, \n@simonpcouch, \n@szimmer, \n@talegari, \n@therealjpetereit, \n@topepo, \n@walkerjameschris, and  \n@ZWael",
    "link": "https://www.tidyverse.org/blog/2025/02/tidymodels-2025-q1/",
    "pubDate": "Thu, 27 Feb 2025 00:00:00 +0000",
    "source": "Tidyverse Blog",
    "category": "R语言"
  },
  {
    "id": "tidyverse",
    "title": "Air, an extremely fast R formatter",
    "description": "We’re thrilled to announce \nAir, an extremely fast R formatter. Formatters are used to automatically style code, but I find that it’s much easier to show what Air can do rather than tell, so we’ll start with a few examples. In the video below, we’re inside \nPositron and we’re looking at some unformatted code. Saving the file (yep, that’s it!) invokes Air, which automatically and instantaneously formats the code.\nNext, let’s go over to \nRStudio. Here we’ve got a pipe chain that could use a little formatting. Like in Positron, just save the file:\nLastly, we’ll jump back into Positron. Rather than formatting a single file on save, you might want to instead format an entire project (particularly when first adopting Air). To do so, just run air format . in a terminal from the project root, and Air will recursively format any R files it finds along the way (smartly excluding known generated files, like cpp11.R). Here we’ll run Air on dplyr for the first time ever, analyzing and reformatting over 150 files instantly:\nWithin the tidyverse, we’re already using Air in some of our largest packages, like \ndplyr, \ntidyr, and \nrecipes.\nThroughout the rest of this post you’ll learn what a formatter is, why you’d want to use one, and you’ll learn a little about how Air decides to format your R code.\nNote that Air is still in beta, so there may be some breaking changes over the next few releases. We also encourage you to use it in combination with a version control system, like git, so that you can clearly see the changes Air makes. That said, we still feel very confident in the current state of Air, and can’t wait for you to try it!\nInstalling Air\n  \n    \n      \n\n      \n\n    \n  \n\nIf you already know how formatters work and want to jump straight in, follow one of the guides below:\nFor Positron, Air is \navailable on OpenVSX as an Extension. Install it from the Extensions pane within Positron, then read our \nPositron guide.\nFor VS Code, Air is \navailable on the VS Code Marketplace as an Extension. Install it from the Extensions pane within VS Code, then read our \nVS Code guide.\nFor RStudio, Air can be set as an external formatter, but you’ll need to install the command line tool for Air first. Read our \nRStudio guide to get started. Note that this is an experimental feature on the RStudio side, so the exact setup may change a little until it is fully stabilized.\nFor command line users, Air binaries can be installed using our \nstandalone installer scripts.\nFor both Positron and VS Code, the most important thing to enable after installing the extension is format on save for R. You can do that by adding these lines to your settings.json file:\n{\n    \"[r]\": {\n        \"editor.formatOnSave\": true\n    }\n}\n\n\nTo open your settings.json file, run one of the following from the Command Palette:\nRun Preferences: Open User Settings (JSON) to modify global user settings.\nRun Preferences: Open Workspace Settings (JSON) to modify project specific settings. You may want to use this instead of setting the user level setting if you drop in on multiple projects, but not all of them use Air. If you work on a project with collaborators, we recommend that you check in these project specific settings to your repository to ensure that every collaborator is using the same formatting settings.\nIf your preferred editor isn’t listed here, but does support the \nLanguage Server Protocol, then it is likely that we can add support for Air there as well.\nIf you have any questions or run into issues installing or using Air, feel free to open an \nissue!\nWhat’s a formatter?\n  \n    \n      \n\n      \n\n    \n  \n\nA formatter is in charge of the layout of your R code. Formatters do not change the meaning of code; instead they ensure that whitespace, newlines, and other punctuation conform to a set of rules and standards, such as:\nMaking sure your code is indented with the appropriate amount of leading whitespace. By default, Air uses 2 spaces for indentation. You will see this indentation in pipelines:\ndata |>\n  ggplot(aes(x, y)) +\n  geom_point()\n\n\nAs well as in function calls:\nlist(\n  foo = 1,\n  bar = 2\n)\n\n\nPreventing your code from overflowing a given line width. By default, Air uses a line width of 80 characters. It enforces this by splitting long lines of code over multiple lines. For instance, notice how long these expressions are, they would “overflow” past 80 characters:\nband_members |> select(name) |> full_join(band_instruments2, by = join_by(name == artist))\n\nleft_join <- function(x, y, by = NULL, copy = FALSE, suffix = c(\".x\", \".y\"), ..., keep = NULL) {\n  UseMethod(\"left_join\")\n}\n\n\nAir reformats these expressions by switching them from a horizontal layout (called “flat”) to a vertical one (called “expanded”):\nband_members |> \n  select(name) |> \n  full_join(band_instruments2, by = join_by(name == artist))\n\nleft_join <- function(\n  x,\n  y,\n  by = NULL,\n  copy = FALSE,\n  suffix = c(\".x\", \".y\"),\n  ...,\n  keep = NULL\n) {\n  UseMethod(\"left_join\")\n}\n\n\nStandardizing the whitespace around code elements. Have you ever had difficulties deciphering very dense code?\n1+2:3*(4/5)\n\n\nAir reformats this expression to:\n1 + 2:3 * (4 / 5)\n\n\nHow does a formatter improve your workflow?\n  \n    \n      \n\n      \n\n    \n  \n\nBy using a formatter it might seem like you’re giving up control over the layout of your code. And indeed you are! However, putting Air in charge of styling your code has substantial advantages.\nFirst, it automatically forces you to write legible code that is neither too wide nor too narrow, with proper breathing room around syntactic elements. Having a formatter as a companion significantly improves the process of writing code as you no longer have to think about style - the formatter does that for you!\nSecond, it reduces friction when working in a team. By agreeing to use a formatter in a project, collaborators no longer have to discuss styling and layout issues. Code sent to you by a colleague will adhere to the standards that you’re used to. Code review no longer has to be about style nitpicks and can focus on the substance of the changes instead.\nHow does Air decide how to format your code?\n  \n    \n      \n\n      \n\n    \n  \n\nAir tries to strike a balance between enforcing rigid rules and allowing authors some control over the layout. Our main source of styling rules is the \nTidyverse style guide, but we occasionally deviate from these.\nThere is a trend among modern formatters of being opinionated. Air certainly fits this trend and provides very few \nconfiguration options, mostly: the indent style (spaces versus tabs), the indent width, and the line width. However, Air also puts code authors in charge of certain aspects of the layout through the notion of persistent line breaks.\nIn general, Air is in control of deciding where to put vertical space (line breaks) in your code. For instance if you write:\ndictionary <- list(bob = \"apple\",\n  jill = \"juice\")\n\n\nAir will figure out that this expression fits on a single line without exceeding the line width. It will discard the line break and reformat to:\ndictionary <- list(bob = \"apple\", jill = \"juice\")\n\n\nHowever there are very specific places at which you can insert a line break that Air perceives as persistent:\nBefore the very first argument in a function call. This:\n# Persistent line break after `(` and before `bob`\ndictionary <- list(\n  bob = \"apple\", jill = \"juice\")\n\n\ngets formatted as:\ndictionary <- list(\n  bob = \"apple\", \n  jill = \"juice\"\n)\n\n\nBefore the very first right-hand side expression in a pipeline. This:\n# Persistent line break after `|>` and before `select`\ndata |>\n  select(foo) |> filter(!bar)\n\n\ngets formatted as:\ndata |>\n  select(foo) |>\n  filter(!bar)\n\n\nA persistent line break will never be removed by Air. But you can remove it manually. Taking the last example, if you join the first lines like this:\n# Removed persistent line break after `(`\ndictionary <- list(bob = \"apple\", \n  jill = \"juice\"\n)\n\n# Removed persistent line break after `|>`\ndata |> select(foo) |>\n  filter(!bar)\n\n\nAir will recognize that you’ve removed the persistent line break, and reformat as:\ndictionary <- list(bob = \"apple\", jill = \"juice\")\n\ndata |> select(foo) |> filter(!bar)\n\n\nThe goal of this feature is to strike a balance between being opinionated and recognizing that users often know when taking up more vertical space results in more readable output.\nHow can I disable formatting?\n  \n    \n      \n\n      \n\n    \n  \n\nIf you need to disable formatting for a single expression, you can use a # fmt: skip comment. This is particularly useful for manual alignment.\n# This skips formatting for `list()` and its arguments, retaining the manual alignment\n# fmt: skip\nlist(\n  dollar = \"USA\",\n  yen    = \"Japan\",\n  yuan   = \"China\"\n)\n\n# This skips formatting for `tribble()` and its arguments\n# fmt: skip\ntribble(\n  ~x, ~y,\n   1,  2,\n)\n\n\nIf there is a file that Air should skip altogether, you can use a # fmt: skip file comment at the very top of the file.\nTo learn more about these features, see the \ndocumentation.\nHow can I use Air?\n  \n    \n      \n\n      \n\n    \n  \n\nAs we’ve touched on above, Air can be integrated into your IDE to format code on every save. We expect that this will be the most common way to invoke Air, but there are a few other ways to use Air that we think are pretty cool:\nIn IDEs:\nFormat on save\nFormat selection\nAt the command line:\nFormat entire projects with air format .\nSet up a git precommit hook to invoke Air before committing\nIn CI:\nUse a GitHub Action to check that each PR conforms to formatting standards with air format . --check1\nUse a GitHub Action to automatically format each PR by pushing the results of air format as a commit\nWe don’t have guides for all of these use cases yet, but the best place to stay up to date is the \nAir website.\nHow is this different from styler?\n  \n    \n      \n\n      \n\n    \n  \n\nAir would not exist without the preexisting work and dedication poured into \nstyler. Created by \nLorenz Walthert and \nKirill Müller, styler proved that the R community does care about how their code is formatted, and has been the primary implementation of the \ntidyverse style guide for many years. We’ve spoken to Lorenz about Air, and we are all very excited about what Air can do for the future of formatting in R.\nAir is different from styler in a few key ways:\nAir is much faster. So much so that it enables new ways of using a formatter that were somewhat painful before, like formatting on every save, or formatting entire projects on every pull request.\nAir is less configurable. As mentioned above, Air provides very few \nconfiguration options.\nAir respects a line width, with a default of 80 characters.\nAir does not require R to run. Unlike styler, which is an R package, Air is written in Rust and is distributed as a pre-compiled binary for many platforms. This makes Air easy to use across IDEs or on CI with very little setup required.\nHow fast is “extremely fast”?\n  \n    \n      \n\n      \n\n    \n  \n\nAir is written in Rust using the formatting infrastructure provided by \nBiome2. This is also the same infrastructure that \nRuff, the fast Python formatter, originally forked from. Both of those projects are admired for their performance, and Air is no exception.\nOne goal for Air is for “format on save” to be imperceptibly fast, encouraging you to keep it on at all times. Benchmarking formatters is a bit hand wavy due to some having built in caching, so bear with me, but one way to proxy this performance is by formatting a large single file, for example the 800+ line \njoin.R in dplyr. Formatting this takes3:\n0.01 seconds with Air\n1 second with styler (no cache)\nSo, ~100x faster for Air! If you make a few changes in the file after the first round of formatting and run the formatter again, then you get something like:\n0.01 seconds with Air\n0.5 seconds with styler (with cache)\nHalf a second for styler might not sound that bad (and indeed, for a formatter written in R it’s pretty good), but it’s slow enough that you’ll “feel” it if you try and invoke styler on every save. But 0.01 seconds? You’ll never even know its running!\nThe differences get even more drastic if you format entire projects. Formatting the ~150 R files in dplyr takes4:\n0.3 seconds with Air\n100 seconds with styler\nOver 300x faster!\nOut of curiosity, we also ran Air over all ~900 R files in base R and it finished in under 2 seconds.\nWrapping up\n  \n    \n      \n\n      \n\n    \n  \n\nBy contributing this formatter to the R community, our objective is threefold:\nVastly improve your enjoyment of writing well-styled R code by removing the chore of editing whitespace.\nReduce friction in collaborative projects by establishing a consistent style once and for all.\nImprove the overall readability of R code for the community.\nWe hope that Air will prove to be a valuable companion in your daily workflow!\nThe Shiny team already has a \nGitHub Action to help with this. We will likely work on refining this and incorporating it more officially into an Air or r-lib repository. ↩︎\nBiome is an open source project maintained by community members, please consider \nsponsoring them! ↩︎\nThese benchmarks were run with air format R/join.R and styler::style_file(\"R/join.R\"). ↩︎\nWith air format . and \nstyler::style_pkg() ↩︎",
    "link": "https://www.tidyverse.org/blog/2025/02/air/",
    "pubDate": "Fri, 21 Feb 2025 00:00:00 +0000",
    "source": "Tidyverse Blog",
    "category": "R语言"
  },
  {
    "id": "tidyverse",
    "title": "Three experiments in LLM code assist with RStudio and Positron",
    "description": "The last few months, I’ve been exploring how AI/LLMs might make my time developing R packages and doing data science more productive. This post will describe three experimental R packages—\npal, \nensure, and \ngander—that came out of that exploration, and the core tools underlying them. Taken together, I’ve found that these packages allow me to automate many of the less interesting parts of my work, turning all sorts of 45-second tasks into 5-second ones. Excitement from folks in the community has been very encouraging so far, and I’m looking forward to getting each of these packages buttoned up and sent off to CRAN in the coming weeks!\nBackground\n  \n    \n      \n\n      \n\n    \n  \n\nTwice a year, the tidyverse team sets a week aside for “spring cleaning,” bringing all of our R packages up to snuff with the most current tooling and standardizing various bits of our development process. Some of these updates can happen by calling a single function, while others are much more involved. One of those more involved updates is updating erroring code, transitioning away from base R (e.g. \nstop()), rlang (e.g. \nrlang::abort()), \nglue, and homegrown combinations of them. cli’s new syntax is easier to work with as a developer and more visually pleasing as a user.\nIn some cases, transitioning is almost as simple as Finding + Replacing \nrlang::abort() to \ncli::cli_abort():\n# before:\nrlang::abort(\"`save_pred` can only be used if the initial results saved predictions.\")\n\n# after: \ncli::cli_abort(\"{.arg save_pred} can only be used if the initial results saved predictions.\")\n\n\nIn others, there’s a mess of ad-hoc pluralization, \npaste0()s, glue interpolations, and other assorted nonsense to sort through:\n# before:\nextra_grid_params <- glue::single_quote(extra_grid_params)\nextra_grid_params <- glue::glue_collapse(extra_grid_params, sep = \", \")\n\nmsg <- glue::glue(\n  \"The provided `grid` has the following parameter columns that have \",\n  \"not been marked for tuning by `tune()`: {extra_grid_params}.\"\n)\n\nrlang::abort(msg)\n\n# after:\ncli::cli_abort(\n  \"The provided {.arg grid} has parameter columns that have not been\n   marked for tuning by {.fn tune}: {.val {extra_grid_params}}.\"\n)\n\n\nTotal pain, especially with thousands upon thousands of error messages thrown across the tidyverse, r-lib, and tidymodels organizations.\nThe week before our most recent spring cleaning, I participated in an internal Posit LLM hackathon, where a small group of employees would familiarize with interfacing with LLMs via APIs and then set aside a day or two to build something to make their work easier. Heading into our spring cleaning and dreading the task of updating thousands of these calls, I decided to look into how effectively LLMs could help me convert this code. Thus was born \nclipal1, a (now-superseded) R package that allows users to select erroring code, press a keyboard shortcut, wait a moment, and watch the updated code be inlined in to the selection.\n\nclipal was a huge boost for us in the most recent spring cleaning. Depending on the code being updated, these erroring calls used to take 30 seconds to a few minutes. With clipal, though, the model could usually get the updated code 80% or 90% of the way there in a couple seconds. Up to this point, irritated by autocomplete and frustrated by the friction of copying and pasting code and typing out the same bits of context into chats again and again, I had been relatively skeptical that LLMs could make me more productive. After using clipal for a week, though, I began to understand how seamlessly LLMs could automate the cumbersome and uninteresting parts of my work.\nclipal itself is now superseded by pal, a more general solution to the problem that clipal solved. I’ve also written two additional packages like pal that solve two other classes of pal-like problems using similar tools, ensure and gander. In this post, I’ll write a bit about how I’ve used a pair of tools in three experiments that have made me much more productive as an R developer.\nPrerequisites: ellmer and the RStudio API\n  \n    \n      \n\n      \n\n    \n  \n\nWhile clipal is now superseded, the package that supersedes it and its other two descendants makes use of the same two tools: \nellmer and the \nRStudio API.\nLast year, Hadley Wickham and Joe Cheng began work on ellmer, a package that aims to make it easy to use large language models in R. For folks that have tried to use LLM APIs through HTTP requests, or interfaced with existing tools that wrap them like langchain, ellmer is pretty incredible. R users can initialize a Chat object using a predictably named function:\nlibrary(ellmer)\n\n# to use a model like GPT-4o or GPT-4o-mini from OpenAI:\nch <- chat_openai()\n\n# ...or a locally hosted ollama model:\nch <- chat_ollama()\n\n# ...or Claude's Sonnet model:\nch <- chat_claude()\n\n\nThen calling the output’s $chat() method returns a character response:\nch$chat(\"When was R created? Be brief.\")\n#> R was created in 1993 by Ross Ihaka and Robert Gentleman at \n#> the University of Auckland, New Zealand.\n\n\nThere’s a whole lot more to ellmer, but this functionality alone was enough to make clipal happen. I could allow users to choose a Chat from whatever provider they prefer to power the addin and ellmer would take care of all of the details underneath the hood.\nThe other puzzle piece here was how to get that character vector directly into the file so that the user didn’t have to copy and paste code from a chat interface into their document. The RStudio IDE supplies an API to interface with various bits of the RStudio UI through R code via the rstudioapi package. Notably, through R code, the package can read what’s inside of the user’s active selection and also write character vectors into that range. clipal could thus:\nWhen triggered, read what’s inside of the selection using rstudioapi.\nPass that selection contents to an LLM along with a system prompt describing how to convert R erroring code to use cli using ellmer. (If you’re curious, the current draft of that prompt is \nhere.)\nWhen the response is returned, replace the contents of the selection with the response using cli.\nThis approach of using ellmer and the rstudioapi has its ups and downs. As for the advantages:\nOur \nPositron IDE has “shims” of the RStudio API, so whatever works in RStudio will also work in Positron. This means that the same shortcuts can be mapped to the same tool in either IDE and it will just work without me, as the developer, having to do anything.2\nSince these packages are written in R, they have access to your R environment. This is quite the differentiator compared to the more language-agnostic tools out there—these packages can see the data frames you have loaded, the columns and column types in them, etc. When working with other tools for LLM code-assist that don’t have this information, the friction of printing out variable information from my R environment and pasting it into whatever interface is so high that I don’t even ask LLMs for help with tasks they’re otherwise totally capable of.\nUsing ellmer under the hood means that, once R users have set up model connections with ellmer, they can use the same configuration with any of these packages with minimal additional effort. So, clipal and the packages that followed it support whatever model providers their users want to use—OpenAI, Claude, local ollama models, and so on. If you can use it with ellmer, you can use it with these packages.\nAs for the disadvantages, there are all sorts of UI bummers about this approach. Above all, these packages write directly to your files. This is great in that it removes the need to copy and paste, and when the model’s response is spot on, it’s awesome. At the same time, if the model starts rambling in an .R file or you want to confirm some difference between your previous code and the new code, the fact that these packages just write right into your files can be a bit annoying. Many other inline LLM code-assist tools out there are based on diffs—they show you proposed changes and some UI element that allows you to accept them, reject them, or ask for revisions. This requires one more step between asking for an LLM to do something and the thing actually being done, but saves the pain of lots of undoing or manually retrieving what code used to look like to verify the model’s work.\npal\n  \n    \n      \n\n      \n\n    \n  \n\nAfter using clipal during our spring cleaning, I approached another spring cleaning task for the week: updating testing code. testthat 3.0.0 was released in 2020, bringing with it numerous changes that were both huge quality of life improvements for package developers and also highly breaking changes. While some of the task of converting legacy unit testing code to testthat 3e is relatively straightforward, other components can be quite tedious. Could I do the same thing for updating to testthat 3e that I did for transitioning to cli? I sloppily threw together a sister package to clipal that would convert tests for errors to snapshot tests, disentangle nested expectations, and transition from deprecated functions like ⁠expect_known_*(). ⁠(If you’re interested, the current prompt for that functionality is \nhere.) That sister package was also a huge boost for me, but the package reused as-is almost every piece of code from clipal other than the prompt. Thus, I realized that the proper solution would provide all of this scaffolding to attach a prompt to a keyboard shortcut, but allow for an arbitrary set of prompts to help automate these wonky, cumbersome tasks.\nThe next week, \npal was born. The pal package ships with three prompts centered on package development: the cli pal and testthat pal mentioned previously, as well as the roxygen pal, which drafts minimal roxygen documentation based on a function definition. Here’s what pal’s interface looks like now:\n\nUsers can add custom prompts for whatever tasks they please and they’ll be included in the searchable dropdown shown above.\nI’ve been super appreciative of all of the love the package has received already, and I’ll be sending the package out to CRAN in the coming weeks.\nensure\n  \n    \n      \n\n      \n\n    \n  \n\nWhile deciding on the initial set of prompts that pal would include, I really wanted to include some sort of “write unit tests for this function” pal. To really address this problem, though, requires violating two of pal’s core assumptions:\nAll of the context that you need is in the selection and the prompt. In the case of writing unit tests, it’s actually pretty important to have other pieces of context. If a package provides some object type potato, in order to write tests for some function that takes potato as input, it’s likely very important to know how potatoes are created and the kinds of properties they have. pal’s sister package for writing unit tests, ensure, can thus “see” the rest of the file that you’re working on, as well as context from neighboring files like other .R source files, the corresponding test file, and package vignettes, to learn about how to interface with the function arguments being tested.\nThe LLM’s response can prefix, replace, or suffix the active selection in the same file. In the case of writing unit tests for R, the place that tests actually ought to go is in a corresponding test file in tests/testthat/. Via the RStudio API, ensure can open up the corresponding test file and write to it rather than the source file where it was triggered from.3\n\nSo far, I haven’t spent as much time with ensure as I have with pal or gander, but I’ll be revisiting the package and sending it off to CRAN in the coming weeks.\ngander\n  \n    \n      \n\n      \n\n    \n  \n\n\npal really excels at things you do all the time. Providing custom prompts with lots of details about code syntax and your taste means that models will often provide code that’s almost exactly what you’d write yourself. On its own, though, pal is incomplete as a toolkit for LLM code-assist. What about one-off requests that are specific to the environment that I’m working in or things I only do every once in a long while? It’s nice to have a much more general tool that functions much more like a chat interface.\nAt the same time, working with usual chat interfaces is quite high-friction, so much so that you’ll likely spend more time pasting in context from your files and R environmet than you would if you had just written the code yourself. There are all sorts of language-agnostic interfaces (or language-specific but not for R or RStudio) tools out there implementing this. You type some request with your cursor near some code, and then, in the backend, the tool assembles a bunch of context that will help the model respond more effectively. This is super helpful for many software engineering contexts, where most all of the context you need can be found in the contents of files. Data science differs a bit from software engineering here, though, in that the state of your R environment is just as important (or more so) than the contents of your files. For example, the lines of your files may show that you reference some data frame called stackoverflow, but what will really help a model write R code to interface with that data frame is “seeing” it: what columns are in it, and what are their types and distributions? gander is a chat interface that allows models to see the data you’re working with.\n\nBehind the scenes, gander combines your selection (or lack thereof), inputted request, file type and contents, and R environment to dynamically assemble prompts to best enable models to tailor their responses to your R session. I use gander several times every day to turn 45-second tasks into 5-second ones and have been super stoked with how well-received it’s been among R folks so far. Compared to pal and ensure, this package feels like a much more substantial lift for data scientists specifically (rather than package developers). In the coming weeks, I’ll sand down some of its rough edges and send it off to CRAN.\nWhat’s next?\n  \n    \n      \n\n      \n\n    \n  \n\nFor now, all of these packages only live on my GitHub profile. In the coming weeks, I plan to revisit each of them, squash a bunch of bugs, and send them off to CRAN.\nThat said, these packages are very much experimental. The user interface of writing directly to users’ files very much limits how useful these tools can be, and I think that the kinds of improvements to interface I’m hoping for may only be possible via some backend other than the RStudio API. I’m looking forward to seeing what that could look like.\nPronounced “c-l-i pal.” ↩︎\nIn reality, there are bugs and differences here and there, but the development effort to get these packages working in Positron was relatively minimal. ↩︎\nThis is one gap between the RStudio API and Positron’s shims for it. The Positron shims currently don’t allow for toggling between files, so ensure isn’t available in Positron. ↩︎",
    "link": "https://www.tidyverse.org/blog/2025/01/experiments-llm/",
    "pubDate": "Wed, 29 Jan 2025 00:00:00 +0000",
    "source": "Tidyverse Blog",
    "category": "R语言"
  },
  {
    "id": "tidyverse",
    "title": "nanoparquet 0.4.0",
    "description": "5x height\n* [x] [`hugodown::use_tidy_thumbnails()`](https://rdrr.io/pkg/hugodown/man/use_tidy_post.html)\n* [x] Add intro sentence, e.g. the standard tagline for the package\n* [x] [`usethis::use_tidy_thanks()`](https://usethis.r-lib.org/reference/use_tidy_thanks.html)\n-->\nWe’re thrilled to announce the release of \nnanoparquet 0.4.0. nanoparquet is an R package that reads and writes Parquet files.\nYou can install it from CRAN with:\ninstall.packages(\"nanoparquet\")\n\n\nThis blog post will show the most important new features of nanoparquet 0.4.0: You can see a full list of changes in the \nrelease notes.\nBrand new read_parquet()\n  \n    \n      \n\n      \n\n    \n  \n\nnanoparquet 0.4.0 comes with a completely rewritten Parquet reader. The new version has an architecture that is easier to embed into R, and facilitates fantastic new features, like \nappend_parquet() and the new col_select argument. (More to come!) The new reader is also much faster, see the “Benchmarks” chapter.\nRead a subset of columns\n  \n    \n      \n\n      \n\n    \n  \n\nread_parquet() now has a new argument called col_select, that lets you read a subset of the columns from the Parquet file. Unlike for row oriented file formats like CSV, this means that the reader never needs to touch the columns that are not needed for. The time required for reading a subset of columns is independent of how many more columns the Parquet file might have!\nYou can either use column indices or column names to specify the columns to read. Here is an example.\nlibrary(nanoparquet)\nlibrary(pillar)\nThis is the \nnycflights13::flights data set:\nread_parquet(\n  \"flights.parquet\",\n  col_select = c(\"dep_time\", \"arr_time\", \"carrier\")\n)\n#> # A data frame: 336,776 × 3\n#>    dep_time arr_time carrier\n#>       <int>    <int> <chr>  \n#>  1      517      830 UA     \n#>  2      533      850 UA     \n#>  3      542      923 AA     \n#>  4      544     1004 B6     \n#>  5      554      812 DL     \n#>  6      554      740 UA     \n#>  7      555      913 B6     \n#>  8      557      709 EV     \n#>  9      557      838 B6     \n#> 10      558      753 AA     \n#> # ℹ 336,766 more rows\n\nUse \nread_parquet_schema() if you want to see the structure of the Parquet file first:\nread_parquet_schema(\"flights.parquet\")\n#> # A data frame: 20 × 12\n#>    file_name       name  r_type type  type_length repetition_type converted_type\n#>    <chr>           <chr> <chr>  <chr>       <int> <chr>           <chr>         \n#>  1 flights.parquet sche… NA     NA             NA NA              NA            \n#>  2 flights.parquet year  integ… INT32          NA REQUIRED        INT_32        \n#>  3 flights.parquet month integ… INT32          NA REQUIRED        INT_32        \n#>  4 flights.parquet day   integ… INT32          NA REQUIRED        INT_32        \n#>  5 flights.parquet dep_… integ… INT32          NA OPTIONAL        INT_32        \n#>  6 flights.parquet sche… integ… INT32          NA REQUIRED        INT_32        \n#>  7 flights.parquet dep_… double DOUB…          NA OPTIONAL        NA            \n#>  8 flights.parquet arr_… integ… INT32          NA OPTIONAL        INT_32        \n#>  9 flights.parquet sche… integ… INT32          NA REQUIRED        INT_32        \n#> 10 flights.parquet arr_… double DOUB…          NA OPTIONAL        NA            \n#> 11 flights.parquet carr… chara… BYTE…          NA REQUIRED        UTF8          \n#> 12 flights.parquet flig… integ… INT32          NA REQUIRED        INT_32        \n#> 13 flights.parquet tail… chara… BYTE…          NA OPTIONAL        UTF8          \n#> 14 flights.parquet orig… chara… BYTE…          NA REQUIRED        UTF8          \n#> 15 flights.parquet dest  chara… BYTE…          NA REQUIRED        UTF8          \n#> 16 flights.parquet air_… double DOUB…          NA OPTIONAL        NA            \n#> 17 flights.parquet dist… double DOUB…          NA REQUIRED        NA            \n#> 18 flights.parquet hour  double DOUB…          NA REQUIRED        NA            \n#> 19 flights.parquet minu… double DOUB…          NA REQUIRED        NA            \n#> 20 flights.parquet time… POSIX… INT64          NA REQUIRED        TIMESTAMP_MIC…\n#> # ℹ 5 more variables: logical_type <I<list>>, num_children <int>, scale <int>,\n#> #   precision <int>, field_id <int>\n\nThe output of \nread_parquet_schema() also shows you the R type that nanoparquet will use for each column.\nAppending to Parquet files\n  \n    \n      \n\n      \n\n    \n  \n\nThe new \nappend_parquet() function makes it easy to append new data to a Parquet file, without first reading the whole file into memory. The schema of the file and the schema new data must match of course. Lets merge \nnycflights13::flights and \nnycflights23::flights:\nfile.copy(\"flights.parquet\", \"allflights.parquet\", overwrite = TRUE)\n#> [1] TRUE\nappend_parquet(nycflights23::flights, \"allflights.parquet\")\nread_parquet_info() returns the most basic information about a Parquet file:\nread_parquet_info(\"flights.parquet\")\n#> # A data frame: 1 × 7\n#>   file_name       num_cols num_rows num_row_groups file_size parquet_version\n#>   <chr>              <int>    <dbl>          <int>     <dbl>           <int>\n#> 1 flights.parquet       19   336776              1   5687737               1\n#> # ℹ 1 more variable: created_by <chr>\nread_parquet_info(\"allflights.parquet\")\n#> # A data frame: 1 × 7\n#>   file_name          num_cols num_rows num_row_groups file_size parquet_version\n#>   <chr>                 <int>    <dbl>          <int>     <dbl>           <int>\n#> 1 allflights.parquet       19   772128              1  13490997               1\n#> # ℹ 1 more variable: created_by <chr>\n\nNote that you should probably still create a backup copy of the original file when using \nappend_parquet(). If the appending process is interrupted by a power failure, then you might end up with an incomplete and invalid Parquet file.\nSchemas and type conversions\n  \n    \n      \n\n      \n\n    \n  \n\nIn nanoparquet 0.4.0 \nwrite_parquet() takes a schema argument that can customize the R to Parquet type mappings. For example by default \nwrite_parquet() writes an R character vector as a STRING Parquet type. If you’d like to write a certain character column as an ENUM type1 instead, you’ll need to specify that in schema:\nwrite_parquet(\n  nycflights13::flights,\n  \"newflights.parquet\",\n  schema = parquet_schema(carrier = \"ENUM\")\n)\nread_parquet_schema(\"newflights.parquet\")\n#> # A data frame: 20 × 12\n#>    file_name       name  r_type type  type_length repetition_type converted_type\n#>    <chr>           <chr> <chr>  <chr>       <int> <chr>           <chr>         \n#>  1 newflights.par… sche… NA     NA             NA NA              NA            \n#>  2 newflights.par… year  integ… INT32          NA REQUIRED        INT_32        \n#>  3 newflights.par… month integ… INT32          NA REQUIRED        INT_32        \n#>  4 newflights.par… day   integ… INT32          NA REQUIRED        INT_32        \n#>  5 newflights.par… dep_… integ… INT32          NA OPTIONAL        INT_32        \n#>  6 newflights.par… sche… integ… INT32          NA REQUIRED        INT_32        \n#>  7 newflights.par… dep_… double DOUB…          NA OPTIONAL        NA            \n#>  8 newflights.par… arr_… integ… INT32          NA OPTIONAL        INT_32        \n#>  9 newflights.par… sche… integ… INT32          NA REQUIRED        INT_32        \n#> 10 newflights.par… arr_… double DOUB…          NA OPTIONAL        NA            \n#> 11 newflights.par… carr… chara… BYTE…          NA REQUIRED        ENUM          \n#> 12 newflights.par… flig… integ… INT32          NA REQUIRED        INT_32        \n#> 13 newflights.par… tail… chara… BYTE…          NA OPTIONAL        UTF8          \n#> 14 newflights.par… orig… chara… BYTE…          NA REQUIRED        UTF8          \n#> 15 newflights.par… dest  chara… BYTE…          NA REQUIRED        UTF8          \n#> 16 newflights.par… air_… double DOUB…          NA OPTIONAL        NA            \n#> 17 newflights.par… dist… double DOUB…          NA REQUIRED        NA            \n#> 18 newflights.par… hour  double DOUB…          NA REQUIRED        NA            \n#> 19 newflights.par… minu… double DOUB…          NA REQUIRED        NA            \n#> 20 newflights.par… time… POSIX… INT64          NA REQUIRED        TIMESTAMP_MIC…\n#> # ℹ 5 more variables: logical_type <I<list>>, num_children <int>, scale <int>,\n#> #   precision <int>, field_id <int>\n\nHere we wrote the carrier column as ENUM, and left the other other columns to use the default type mappings.\nSee the \n?nanoparquet-types manual page for the possible type mappings (lots of new ones!) and also for the default ones.\nEncodings\n  \n    \n      \n\n      \n\n    \n  \n\nIt is now also possible to customize the encoding of each column in \nwrite_parquet(), via the encoding argument. By default \nwrite_parquet() tries to choose a good encoding based on the type and the values of a column. E.g. it checks a small sample for repeated values to decide if it is worth using a dictionary encoding (RLE_DICTIONARY).\nIf \nwrite_parquet() gets it wrong, use the encoding argument to force an encoding. The following forces the PLAIN encoding for all columns. This encoding is very fast to write, but creates a larger file. You can also specify different encodings for different columns, see the \nwrite_parquet() manual page.\nwrite_parquet(\n  nycflights13::flights,\n  \"plainflights.parquet\",\n  encoding = \"PLAIN\"\n)\nfile.size(\"flights.parquet\")\n#> [1] 5687737\nfile.size(\"plainflights.parquet\")\n#> [1] 11954574\n\nSee more about the implemented encodings and how the defaults are selected in the \nparquet-encodings manual page.\nAPI changes\n  \n    \n      \n\n      \n\n    \n  \n\nSome nanoparquet functions have new, better names in nanoparquet 0.4.0. In particular, all functions that read from a Parquet file have a read_parquet prefix now. The old functions still work, with a warning.\nAlso, the \nparquet_schema() function is now for creating a new Parquet schema from scratch, and not for inferring a schema from a data frame (use \ninfer_parquet_schema()) or for reading the schema from a Parquet file (use \nread_parquet_schema()). \nparquet_schema() falls back to the old behaviour when called with a file name, with a warning, so this is not a breaking change (yet), and old code still works.\nSee the complete list of API changes in the \nChangelog.\nBenchmarks\n  \n    \n      \n\n      \n\n    \n  \n\nWe are very excited about the performance of the new Parquet reader, and the Parquet writer was always quite speedy, so we ran a simple benchmark.\nWe compared nanoparquet to the Parquet implementations in Apache Arrow and DuckDB, and also to CSV readers and writers, on a real data set, for samples of 330k, 6.7 million and 67.4 million rows (40MB, 800MB and 8GB in memory). For these data nanoparquet is indeed very competitive with both Arrow and DuckDB.\nYou can see the full results \non the website.\nOther changes\n  \n    \n      \n\n      \n\n    \n  \n\nOther important changes in nanoparquet 0.4.0 include:\nwrite_parquet() can now write multiple row groups. By default it puts at most 10 million rows in a single row group. (This is subject to https://nanoparquet.r-lib.org/references/parquet_options.html ) on how to change the default.\nwrite_parquet() now writes minimum and maximum statistics (by default) for most Parquet types. See the \nparquet_options() manual page on how to turn this off, which will probably make the writer faster.\nwrite_parquet() can now write version 2 data pages. The default is still version 1, but it might change in the future.\nNew compression_level option to select the compression level manually.\nread_parquet() can now read from an R connection.\nAcknowledgements\n  \n    \n      \n\n      \n\n    \n  \n\n@alvarocombo, \n@D3SL, \n@gaborcsardi, and \n@RealTYPICAL.\nA Parquet ENUM type is very similar to a factor in R. ↩︎",
    "link": "https://www.tidyverse.org/blog/2025/01/nanoparquet-0-4-0/",
    "pubDate": "Tue, 28 Jan 2025 00:00:00 +0000",
    "source": "Tidyverse Blog",
    "category": "R语言"
  },
  {
    "id": "tidyverse",
    "title": "httr2 1.1.0",
    "description": "5x height\n* [x] [`hugodown::use_tidy_thumbnails()`](https://rdrr.io/pkg/hugodown/man/use_tidy_post.html)\n* [x] Add intro sentence, e.g. the standard tagline for the package\n* [x] [`usethis::use_tidy_thanks()`](https://usethis.r-lib.org/reference/use_tidy_thanks.html)\n-->\nWe’re chuffed to announce the release of \nhttr2 1.1.0. httr2 (pronounced “hitter2”) is a comprehensive HTTP client that provides a modern, pipeable API for working with web APIs. It builds on top of \n{curl} to provide features like explicit request objects, built-in rate limiting & retry tooling, comprehensive OAuth support, and secure handling of secrets and credentials.\nIn this blog post, we’ll dive into the new streaming interface built around \nreq_perform_connection(), explore the new suite of URL manipulation tools, and highlight a few of the other biggest changes (including better support for AWS and enhancements to the caching system), and update you on the lifecycle changes.\nThis blog post includes the most important enhacenments in versions 1.0.1 through 1.0.7, where we’ve been iterating on various features and fixing numerous bugs. For a complete list of changes, you can check the \nGitHub release notes or the \nNEWS file.\nInstallation\n  \n    \n      \n\n      \n\n    \n  \n\nInstall httr2 from CRAN with:\ninstall.packages(\"httr2\")\nStreaming data\n  \n    \n      \n\n      \n\n    \n  \n\nThe headline feature of this release is a better API for streaming responses, where the body is not available immediately but is streamed back over time. This is particularly important for interacting with LLMs, where it’s needed to make chat responses feel snappy. You can try it out in \nellmer, our new package for chatting with LLMs from a variety of providers.\nThe most important new function is \nreq_perform_connection(), which supersedes the older callback-based \nreq_perform_stream(). Unlike its predecessor, \nreq_perform_connection() returns a regular response object with a connection object for the body:\nlibrary(httr2)\n\nreq <- request(example_url()) |> req_template(\"/stream-bytes/:n\", n = 10240)\nresp <- req_perform_connection(req)\nresp\n#> <httr2_response>\n#> GET http://127.0.0.1:49283/stream-bytes/10240\n#> Status: 200 OK\n#> Content-Type: application/octet-stream\n#> Body: Streaming connection\n\nOnce you have a streaming connection you can repeatedly call a resp_stream_*() function to pull down data in chunks, using \nresp_stream_is_complete() to figure out when to stop.\nwhile (!resp_stream_is_complete(resp)) {\n  bytes <- resp_stream_raw(resp, kb = 2)\n  cat(\"Downloaded \", length(bytes), \" bytes\\n\", sep = \"\")\n}\n#> Downloaded 2048 bytes\n#> Downloaded 2048 bytes\n#> Downloaded 2048 bytes\n#> Downloaded 2048 bytes\n#> Downloaded 2048 bytes\n#> Downloaded 0 bytes\n\nAs well as \nresp_stream_raw(), which returns a raw vector, you can use \nresp_stream_lines() to stream lines and \nresp_stream_sse() to stream \nserver-sent events.\nURL manipulation tools\n  \n    \n      \n\n      \n\n    \n  \n\nWorking with URLs got easier with three new functions: \nurl_modify(), \nurl_modify_query(), and \nurl_modify_relative(). You can see how they work in the examples below:\n# url_modify() modifies components of a URL\nurl_modify(\"https://example.com\", hostname = \"github.com\")\n#> [1] \"https://github.com/\"\nurl_modify(\"https://example.com\", scheme = \"http\")\n#> [1] \"http://example.com/\"\nurl_modify(\"https://example.com\", path = \"abc\", query = list(foo = \"bar\"))\n#> [1] \"https://example.com/abc?foo=bar\"\n\n# url_modify_query() lets you modify individual query parameters\n# modifying an existing parameter:\nurl_modify_query(\"http://example.com?a=1&b=2\", a = 10)\n#> [1] \"http://example.com/?b=2&a=10\"\n# delete a parameter:\nurl_modify_query(\"http://example.com?a=1&b=2\", b = NULL)\n#> [1] \"http://example.com/?a=1\"\n# add a new parameter:\nurl_modify_query(\"http://example.com?a=1&b=2\", c = 3)\n#> [1] \"http://example.com/?a=1&b=2&c=3\"\n\n# url_modify_relative() navigates to a relative URL\nurl_modify_relative(\"https://example.com/a/b/c.html\", \"/d/e/f.html\")\n#> [1] \"https://example.com/d/e/f.html\"\nurl_modify_relative(\"https://example.com/a/b/c.html\", \"C.html\")\n#> [1] \"https://example.com/a/b/C.html\"\nurl_modify_relative(\"https://example.com/a/b/c.html\", \"../B.html\")\n#> [1] \"https://example.com/a/B.html\"\n\nWe also added \nreq_url_relative() to make it easier to navigate to a relative URL for an existing request.\nOther improvements\n  \n    \n      \n\n      \n\n    \n  \n\nThere are a handful of other improvements that are worth highlighting:\nWe’ve made it easier to talk to AWS web services with \nreq_auth_aws_v4() for signing requests and \nresp_stream_aws() for streaming responses. Special thanks goes to the \nlifion-aws-event-stream project for providing a clear reference implementation.\nWe’ve run-down a long list of bugs that made \nreq_cache() unreliable. This includes improving the handling of header-only changes, better cache pruning, and new debugging options. If you’re working with a web API that supports caching, we highly recommend that you try it out. The next release of {\ngh} will use a cache by default, and my use of the dev version suggests that it gives a pretty nice performance improvment.\nis_online() provides an easy way to check internet connectivity.\nreq_perform_promise() allows you to execute requests in the background (thanks to \n@gergness) using an efficient approach that waits on curl socket activity (thanks to \n@shikokuchuo).\nBreaking changes\n  \n    \n      \n\n      \n\n    \n  \n\nAs httr2 continues to mature, we’ve made some lifecycle changes:\nreq_perform_iterative() is now stable and no longer experimental.\nreq_perform_stream() is superseded by \nreq_perform_connection(), as mentioned above.\nwith_mock() and \nlocal_mock() are defunct and will be rmeoved in the next release. Use \nwith_mocked_responses() and \nlocal_mocked_responses() instead.\nAcknowledgements\n  \n    \n      \n\n      \n\n    \n  \n\nA big thanks to all 76 folks who filed issues, created PRs and generally helped to make httr2 better! \n@Aariq, \n@AGeographer, \n@amael-ls, \n@anishjoni, \n@asadow, \n@atheriel, \n@awpsoras, \n@billsanto, \n@bonushenricus, \n@botan, \n@burgerga, \n@CareCT, \n@cderv, \n@cole-brokamp, \n@covid19ec, \n@datapumpernickel, \n@denskh, \n@deschen1, \n@DyfanJones, \n@erydit, \n@exetico, \n@fh-mthomson, \n@frzambra, \n@gergness, \n@GreenGrassBlueOcean, \n@guslipkin, \n@hadley, \n@i2z1, \n@isachng93, \n@IshuaWang, \n@JamesHWade, \n@jameslairdsmith, \n@JBGruber, \n@jcheng5, \n@jeroen, \n@jimbrig, \n@jjesusfilho, \n@jl5000, \n@jmuhlenkamp, \n@jonthegeek, \n@JosiahParry, \n@jwimberl, \n@krjaworski, \n@m-muecke, \n@maarten-vermeyen, \n@MarekGierlinski, \n@maxsutton, \n@mgirlich, \n@MichaelChirico, \n@mkoohafkan, \n@MSHelm, \n@mstei4176, \n@mthomas-ketchbrook, \n@NateNohling, \n@nick-youngblut, \n@pbulsink, \n@PietrH, \n@pkautio, \n@plietar, \n@pmlefeuvre-met, \n@rkrug, \n@romainfrancois, \n@salim-b, \n@shikokuchuo, \n@simplyalexander, \n@sluga, \n@stefanedwards, \n@steveputman, \n@tebancr, \n@thohan88, \n@tony2015116, \n@toobiwankenobi, \n@verhovsky, \n@walinchus, \n@werkstattcodes, and \n@zacdav-db.",
    "link": "https://www.tidyverse.org/blog/2025/01/httr2-1-1-0/",
    "pubDate": "Mon, 20 Jan 2025 00:00:00 +0000",
    "source": "Tidyverse Blog",
    "category": "R语言"
  },
  {
    "id": "tidyverse",
    "title": "Updates to Text Rendering in R Graphics",
    "description": "5x height\n* [x] `hugodown::use_tidy_thumbnails()`\n* [ ] Add intro sentence, e.g. the standard tagline for the package\n* [ ] `usethis::use_tidy_thanks()`\n-->\n\ntext rendering is one of those disciplines where, if you think you finally got it right, you can be 100% certain that you didn't\n— Thomas Lin Pedersen (@thomasp85.com) January 10, 2025 at 10:44 AM\n\nNo reason to hide the fact: Text rendering is complicated! When I set out to improve the support for modern text rendering features in R all those years ago, I don’t think I truly appreciated that fact. And probably for the better, since I’m not sure I would have taken on the task had I known.\nTaking the quote above as a universal truth (it comes from a reputable source after all), I’m sure I’ll never be fully done, but recent work on the whole stack at least makes me worry less about the correctness. This post will go through the changes that span the \nsystemfonts, \ntextshaping, and \nmarquee packages and let you now how you, as a user or developer, should take advantage of them.\nWorking with non-installed fonts\n  \n    \n      \n\n      \n\n    \n  \n\nThe genesis of the systemfonts package was a need to be able to tap into the operating systems font library, so that whatever was installed on the system, would be available to graphics devices (assuming those devices used systemfonts). The scope of the package has gradually increased, and one of the needs that has become obvious over time, is a way to work with fonts, that aren’t installed on the system (E.g. if you want to bundle a font with a package, or if you are deploying a Shiny app that uses a specific font for the graphics).\nUntil now, the register_font() and register_variant() functions have been the only option for letting systemfonts know about fonts other than those installed on the system. However, both of these functions were designed to circumvent limitations in the R graphics system when it comes to font selection (e.g. no way to use a “thin” font variant as the only weight option in the graphics system is bold yes/no), and as such were clunky to use for introducing new fonts.\nWith the new version of systemfonts we get a dedicated way to tell systemfonts “please consider these font files as equals to the installed ones”. The function is called add_fonts() and all you need to do is to pass in a vector of paths to font files and these will then be reachable by systemfonts.\n# Add fonts from specific files\nsystemfonts::add_fonts(c(\"path/to/font1.ttf\", \"path/to/font2.ttf\"))\n\n\nIn addition to this function, systemfonts also comes with scan_local_fonts() that looks in ./fonts and ~/fonts and adds any fonts located there. The function is called when systemfonts is loaded meaning that you can immediately uses fonts saved in these directories. This is great for deploying projects because all you need to do is to include a fonts folder at the root of you project and these fonts will then always be available wherever you deploy your code.\nWhile it is nice to have good access to the font files on your computer, the files has to come from somewhere. Nowadays that somewhere is usually \nGoogle Fonts or some other online font repository. systemfonts is now aware of a few of these repositories (Google Fonts and \nFont Squirrel for now), and can search and download from these (using search_web_fonts(), get_from_google_fonts(), and get_from_font_squirrel()). The downloaded fonts are automatically added using add_fonts() so they are immediately available, and by default they are placed in ~/fonts so that they persist across R sessions and projects.\n# Search and download fonts\nsystemfonts::get_from_font_squirrel(\"Quicksand\")\nsystemfonts::get_from_google_fonts(\"Rubik Moonrocks\")\n\n\nBut what if you don’t want to think too much about all these details and just want to ensure that some specific font is available when a piece of code is running? In that case require_font() got you covered. This function allows you to state a dependency on a font in a script. The function scans the available fonts on the system and, if it doesn’t find anything, proceeds to look for the font in the online repositories, downloading it if it finds it. If that also fails the function will either throw an error, or map the required font to a fallback of your choosing:\nlibrary(systemfonts)\nrequire_font(\"Rubik Moonrocks\")\n\nplot.new()\ntext(0.5, 0.5, \"Fancy Font\", family = \"Rubik Moonrocks\", cex = 6)\n\n\n\nRemember that all of these niceties only goes into effect if you use a graphics device that uses systemfonts. For now, that more or less means that you should use ragg (you should use ragg anyway so that is not a terrible requirement).\nGetting to the Glyphs\n  \n    \n      \n\n      \n\n    \n  \n\nMost fonts these days are based on a vector outline. That means that they can be scaled smoothly to any size and doesn’t take up a lot of storage space. It also means that there are polygons inside the font files and that these can be extracted! This is now possible with systemfonts and the new glyph_outline() and glyph_raster() functions.\n# Get the location of the glyph inside the font\nmoonrocks <- font_info(\"Rubik Moonrocks\")\nG <- glyph_info(\"G\", path = moonrocks$path, index = moonrocks$index)\n\n# Extract the outline of the glyph and plot it\noutline <- glyph_outline(G$index, moonrocks$path, moonrocks$index, size = 400)\ngrid::grid.path(\n  x = outline$x,\n  y = outline$y + 20, # To raise the baseline a bit\n  id = outline$contour,\n  default.units = \"bigpts\",\n  gp = grid::gpar(fill = \"grey\", col = \"black\", lwd = 4)\n)\n\n\n\nExtracting them as polygons means that we can do all sorts of weird stuff with them if we so pleases:\n# Skew the glyph making it italic\ngrid::grid.path(\n  x = outline$x + outline$y * 0.4,\n  y = outline$y + 20, # To raise the baseline a bit\n  id = outline$contour,\n  default.units = \"bigpts\",\n  gp = grid::gpar(fill = \"grey\", col = \"black\", lwd = 4)\n)\n\n\n\n(real italic glyphs are designed to look good skewed, not just skewed versions of the regular glyphs)\nRemember how I said “most fonts” in the beginning of this section. There are still fonts that do not provide an outline, the prime example being most emoji fonts. The glyphs in such fonts are encoded as multiple bitmaps at fixed sizes (Microsofts emoji font going a different way by encoding them as SVGs). Since we can’t get to the data as outlines we can instead extract it as a raster:\nemoji <- font_info(\"emoji\")\ndancer <- glyph_info(\"💃\", path = emoji$path, index = emoji$index)\nraster <- glyph_raster(dancer$index, emoji$path, emoji$index, size = 400)\ngrid::grid.draw(glyph_raster_grob(raster[[1]], 0, 50))\n\n\n\nIn the above we used the glyph_raster_grob() helper function to create a raster grob with the correct scaling of the resulting raster.\nRaster extraction is not only for bitmap encoded fonts since it is easy to go from an outline to a raster (but not the other way around). Freetype (which systemfonts uses) includes a very efficient scanline rasterizer (the same as used in ragg) and we can thus get a raster version of any font:\nraster2 <- glyph_raster(G$index, moonrocks$path, moonrocks$index, size = 400)\ngrid::grid.draw(glyph_raster_grob(raster2[[1]], 0, 20))\n\n\n\nThe Way the Text Flows\n  \n    \n      \n\n      \n\n    \n  \n\nThe thing that provoked me to writing the quote in the beginning of this blog post, was my work on the textshaping package. This package is largely invisible to the user but together with systemfonts it is responsible for laying out strings of text correctly. It figures out the location of every glyph and finds alternative fonts if the selected one doesn’t contain the needed glyph. textshaping powers ragg as well as marquee, doing the heavy lifting of translating a string of text into glyphs and locations.\nPart of converting a string into glyphs and coordinates (a process known as text shaping) is to figure out which way the text flows and act accordingly. For many people left-to-right flow is the natural text direction, but this is merely a cultural bias and many scripts with a different flow exists (arabic and hebrew being the two most dominant right-to-left flowing scripts). So, part of shaping requires figuring out what script a specific character belongs to and what direction it flows. This is all fairly simple when a string internally agrees on the direction of flow, but can get much more complicated when scripts are embedded within other scripts that doesn’t have the same flow (not to mention scripts embedded even deeper). Combine all of this with soft wrapping of text inside an embedded script and you got the recipe for a headache. textshaping (through me) already made the claim that it fully supported bi-directional text but it turned out that I severely misjudged the complexity. Because of this, the shaping engine has been rewritten almost from scratch. Based on the starting quote I can’t quite claim that it now works 100% correctly but it does pass all 91.707 test cases for bidirectional text provided by the Unicode consortium so there’s that.\nAgain, it is unlikely that you will come into contact with textshaping directly so you will mostly experience these improvements in the way text just appears more correct (to the extend that this was ever an issue for you). The place you are most likely to stumble upon these changes is marquee, which uses textshaping under the hood. Styling in marquee has been expanded to include a text_direction setting. It defaults to \"auto\" which mean “deduce it from the text you get”, but you can also set it to \"ltr\" or \"rtl\" to set the direction explicitly. Be aware that this setting doesn’t change how single glyphs flow so you cannot use it to e.g. write arabic in left-to-right flow. Instead it governs the paragraph-level direction and thus how bi-directional text should be assembled. It also governs to which side indentation happen and the placement of bullets in bullet lists. Often, leaving it on the default value will work fine. There are also two new values for the align setting. \"auto\" picks either \"left\" or \"right\" depending on the text direction, while \"justified\" picks either \"justified-left\" or \"justified-right\". This makes it much easier to work natively with right-to-left text as everything just looks as it should. To top it off, classic_style() gains an ltr argument that controls whether the styling in general should cater to left-to-right or right-to-left text. It controls things such as the position of the grey bar in quotation blocks and the indentation of nested lists.\nlibrary(marquee)\n# Create a style specific for rtl text\nrtl_style <- classic_style(\n  text_direction = \"rtl\", # Forces bidi text to be assembled from right to left\n  align = \"auto\", # Will convert itself to \"right\"\n  ltr = FALSE # Will move bullet padding and bar along quote blocks to the right\n)\n\n\n\n\n\nA marquee for Everyone\n  \n    \n      \n\n      \n\n    \n  \n\nSpeaking of marquee, the biggest obstacle it has put in front of its users is that it is build on very new features in R. The ability to write text by placing glyphs one at a time was only added in R 4.2 and not every graphics device supports it yet (worse still, the implementation in the default macOS quartz device caused the session to crash). Again, ragg is your friend, but the Cairo devices also has excellent support.\nText rendering, however, should always work. It is quite frustrating for text to not show up when you expect it to. Because of this it has been a clear plan to expand the support for marquee somehow. With the new version of marquee this is finally a reality. How does it work? Well, remember when we talked about extracting glyph outlines and rasters? If marquee encounters a graphics device that doesn’t provide the necessary features it will take matters into its own hands, by extracting all the necessary polygons and bitmaps and plot them. It is certainly not faster than relying on the optimized routines of the graphics device and it can also lead to visual degradation at smaller font sizes. But it works - everywhere.\nTo show it off, here is an svg created with svglite which doesn’t have the required new features:\ntext <- \"_Fancy_ {.red Font}📝\"\n\nm_grob <- marquee_grob(\n  text,\n  classic_style(\n    body_font = \"Rubik Moonrocks\",\n    base_size = 72\n  )\n)\n\ns <- svglite::svgstring(width = 7, height = 1.5)\ngrid::grid.draw(m_grob)\ninvisible(dev.off())\n\ns()\n\n\n\n\n\n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIf you inspect the SVG above you’ll see that rather than being made up of text elements it is a collection of path and image elements.\nAgain, it is unlikely that many people will use marquee like this. It is much more likely that they will encounter it through ggplot2 in the form of geom_marquee() and element_marquee(). The takeaway, however, is the same - it is now safe to use marquee even when you don’t know which graphics device will be used to render the text with.\nWhat’s Next?\n  \n    \n      \n\n      \n\n    \n  \n\nCircling back to the starting quote. I’m 100% certain I’m not done yet. I believe the next big push will be proper support for vertical text in textshaping (it currently only deals with horizontal text). I also have some plans to get marquee to automatically translate the numbers in ordered lists into their proper representation in the script that is being used, so that e.g. ‘3.’ will be shown as ‘.٣’ when used with Arabic text.",
    "link": "https://www.tidyverse.org/blog/2025/01/text-rendering-updates/",
    "pubDate": "Fri, 17 Jan 2025 00:00:00 +0000",
    "source": "Tidyverse Blog",
    "category": "R语言"
  },
  {
    "id": "tidyverse",
    "title": "orbital 0.3.0",
    "description": "5x height\n* [x] [`hugodown::use_tidy_thumbnails()`](https://rdrr.io/pkg/hugodown/man/use_tidy_post.html)\n* [x] Add intro sentence, e.g. the standard tagline for the package\n* [x] [`usethis::use_tidy_thanks()`](https://usethis.r-lib.org/reference/use_tidy_thanks.html)\n-->\nWe’re thrilled to announce the release of \norbital 0.3.0. orbital lets you predict in databases using tidymodels workflows.\nYou can install it from CRAN with:\ninstall.packages(\"orbital\")\nThis blog post will cover the highlights, which are classification support and the new augment method.\nYou can see a full list of changes in the \nrelease notes.\nClassification support\n  \n    \n      \n\n      \n\n    \n  \n\nThe biggest improvement in this version is that \norbital() now works for supported classification models. See \nvignette for list of all supported models.\nLet’s start by fitting a classification model on the penguins data set, using {xgboost} as the engine.\nrec_spec <- recipe(species ~ ., data = penguins) |>\n  step_unknown(all_nominal_predictors()) |>\n  step_dummy(all_nominal_predictors()) |>\n  step_impute_mean(all_numeric_predictors()) |>\n  step_zv(all_predictors())\n\nlr_spec <- boost_tree() |>\n  set_mode(\"classification\") |>\n  set_engine(\"xgboost\")\n\nwf_spec <- workflow(rec_spec, lr_spec)\nwf_fit <- fit(wf_spec, data = penguins)\nWith this fitted workflow object, we can call \norbital() on it to create an orbital object.\norbital_obj <- orbital(wf_fit)\norbital_obj\n#> \n#> ── orbital Object ──────────────────────────────────────────────────────────────\n#> • island = dplyr::if_else(is.na(island), \"unknown\", island)\n#> • sex = dplyr::if_else(is.na(sex), \"unknown\", sex)\n#> • island_Dream = as.numeric(island == \"Dream\")\n#> • island_Torgersen = as.numeric(island == \"Torgersen\")\n#> • sex_male = as.numeric(sex == \"male\")\n#> • sex_unknown = as.numeric(sex == \"unknown\")\n#> • bill_length_mm = dplyr::if_else(is.na(bill_length_mm), 43.92193, bill_l ...\n#> • bill_depth_mm = dplyr::if_else(is.na(bill_depth_mm), 17.15117, bill_dep ...\n#> • flipper_length_mm = dplyr::if_else(is.na(flipper_length_mm), 201, flipp ...\n#> • body_mass_g = dplyr::if_else(is.na(body_mass_g), 4202, body_mass_g)\n#> • island_Dream = dplyr::if_else(is.na(island_Dream), 0.3604651, island_Dr ...\n#> • island_Torgersen = dplyr::if_else(is.na(island_Torgersen), 0.1511628, i ...\n#> • sex_male = dplyr::if_else(is.na(sex_male), 0.4883721, sex_male)\n#> • sex_unknown = dplyr::if_else(is.na(sex_unknown), 0.03197674, sex_unknow ...\n#> • Adelie = 0 + dplyr::case_when((bill_depth_mm < 15.1 | is.na(bill_depth_ ...\n#> • Chinstrap = 0 + dplyr::case_when((island_Dream < 0.5 | is.na(island_Dre ...\n#> • Gentoo = 0 + dplyr::case_when((bill_depth_mm < 15.95 | is.na(bill_depth ...\n#> • .pred_class = dplyr::case_when(Adelie > Chinstrap & Adelie > Gentoo ~ \" ...\n#> ────────────────────────────────────────────────────────────────────────────────\n#> 18 equations in total.\n\nThis object contains all the information that is needed to produce predictions. Which we can produce with \npredict().\npredict(orbital_obj, penguins)\n#> # A tibble: 344 × 1\n#>    .pred_class\n#>    <chr>      \n#>  1 Adelie     \n#>  2 Adelie     \n#>  3 Adelie     \n#>  4 Adelie     \n#>  5 Adelie     \n#>  6 Adelie     \n#>  7 Adelie     \n#>  8 Adelie     \n#>  9 Adelie     \n#> 10 Adelie     \n#> # ℹ 334 more rows\n\nThe main thing to note here is that the orbital package produces character vectors instead of factors. This is done as a unifying approach since many databases don’t have factor types.\nSpeaking of databases, you can \npredict() on an orbital object using tables from databases. Below we create an ephemeral in-memory RSQLite database.\nlibrary(DBI)\nlibrary(RSQLite)\n\ncon_sqlite <- dbConnect(SQLite(), path = \":memory:\")\npenguins_sqlite <- copy_to(con_sqlite, penguins, name = \"penguins_table\")\nAnd we can predict with it like normal. All the calculations are sent to the database for execution.\npredict(orbital_obj, penguins_sqlite)\n#> # Source:   SQL [?? x 1]\n#> # Database: sqlite 3.47.1 []\n#>    .pred_class\n#>    <chr>      \n#>  1 Adelie     \n#>  2 Adelie     \n#>  3 Adelie     \n#>  4 Adelie     \n#>  5 Adelie     \n#>  6 Adelie     \n#>  7 Adelie     \n#>  8 Adelie     \n#>  9 Adelie     \n#> 10 Adelie     \n#> # ℹ more rows\n\nThis works the same with \nmany types of databases.\nClassification is different from regression in part because it comes with multiple prediction types. The above example showed the default which is hard classification. You can set the type of prediction you want with the type argument to orbital. For classification models, possible options are \"class\" and \"prob\".\norbital_obj_prob <- orbital(wf_fit, type = c(\"class\", \"prob\"))\norbital_obj_prob\n#> \n#> ── orbital Object ──────────────────────────────────────────────────────────────\n#> • island = dplyr::if_else(is.na(island), \"unknown\", island)\n#> • sex = dplyr::if_else(is.na(sex), \"unknown\", sex)\n#> • island_Dream = as.numeric(island == \"Dream\")\n#> • island_Torgersen = as.numeric(island == \"Torgersen\")\n#> • sex_male = as.numeric(sex == \"male\")\n#> • sex_unknown = as.numeric(sex == \"unknown\")\n#> • bill_length_mm = dplyr::if_else(is.na(bill_length_mm), 43.92193, bill_l ...\n#> • bill_depth_mm = dplyr::if_else(is.na(bill_depth_mm), 17.15117, bill_dep ...\n#> • flipper_length_mm = dplyr::if_else(is.na(flipper_length_mm), 201, flipp ...\n#> • body_mass_g = dplyr::if_else(is.na(body_mass_g), 4202, body_mass_g)\n#> • island_Dream = dplyr::if_else(is.na(island_Dream), 0.3604651, island_Dr ...\n#> • island_Torgersen = dplyr::if_else(is.na(island_Torgersen), 0.1511628, i ...\n#> • sex_male = dplyr::if_else(is.na(sex_male), 0.4883721, sex_male)\n#> • sex_unknown = dplyr::if_else(is.na(sex_unknown), 0.03197674, sex_unknow ...\n#> • Adelie = 0 + dplyr::case_when((bill_depth_mm < 15.1 | is.na(bill_depth_ ...\n#> • Chinstrap = 0 + dplyr::case_when((island_Dream < 0.5 | is.na(island_Dre ...\n#> • Gentoo = 0 + dplyr::case_when((bill_depth_mm < 15.95 | is.na(bill_depth ...\n#> • .pred_class = dplyr::case_when(Adelie > Chinstrap & Adelie > Gentoo ~ \" ...\n#> • norm = exp(Adelie) + exp(Chinstrap) + exp(Gentoo)\n#> • .pred_Adelie = exp(Adelie) / norm\n#> • .pred_Chinstrap = exp(Chinstrap) / norm\n#> • .pred_Gentoo = exp(Gentoo) / norm\n#> ────────────────────────────────────────────────────────────────────────────────\n#> 22 equations in total.\n\nNotice how we can select both \"class\" and \"prob\". The predictions now include both hard and soft class predictions.\npredict(orbital_obj_prob, penguins)\n#> # A tibble: 344 × 4\n#>    .pred_class .pred_Adelie .pred_Chinstrap .pred_Gentoo\n#>    <chr>              <dbl>           <dbl>        <dbl>\n#>  1 Adelie             0.989         0.00554      0.00560\n#>  2 Adelie             0.989         0.00554      0.00560\n#>  3 Adelie             0.989         0.00554      0.00560\n#>  4 Adelie             0.709         0.0245       0.267  \n#>  5 Adelie             0.989         0.00554      0.00560\n#>  6 Adelie             0.989         0.00554      0.00560\n#>  7 Adelie             0.989         0.00554      0.00560\n#>  8 Adelie             0.989         0.00554      0.00560\n#>  9 Adelie             0.979         0.00549      0.0158 \n#> 10 Adelie             0.980         0.00559      0.0148 \n#> # ℹ 334 more rows\n\nThat works equally well in databases.\npredict(orbital_obj_prob, penguins_sqlite)\n#> # Source:   SQL [?? x 4]\n#> # Database: sqlite 3.47.1 []\n#>    .pred_class .pred_Adelie .pred_Chinstrap .pred_Gentoo\n#>    <chr>              <dbl>           <dbl>        <dbl>\n#>  1 Adelie             0.989         0.00554      0.00560\n#>  2 Adelie             0.989         0.00554      0.00560\n#>  3 Adelie             0.989         0.00554      0.00560\n#>  4 Adelie             0.709         0.0245       0.267  \n#>  5 Adelie             0.989         0.00554      0.00560\n#>  6 Adelie             0.989         0.00554      0.00560\n#>  7 Adelie             0.989         0.00554      0.00560\n#>  8 Adelie             0.989         0.00554      0.00560\n#>  9 Adelie             0.979         0.00549      0.0158 \n#> 10 Adelie             0.980         0.00559      0.0148 \n#> # ℹ more rows\n\nNew augment method\n  \n    \n      \n\n      \n\n    \n  \n\nThe users of tidymodels have found the \naugment() function to be a handy tool. This function performs predictions and returns them alongside the original data set.\nThis release adds \naugment() support for orbital objects.\naugment(orbital_obj, penguins)\n#> # A tibble: 344 × 8\n#>    .pred_class species island    bill_length_mm bill_depth_mm flipper_length_mm\n#>    <chr>       <fct>   <fct>              <dbl>         <dbl>             <int>\n#>  1 Adelie      Adelie  Torgersen           39.1          18.7               181\n#>  2 Adelie      Adelie  Torgersen           39.5          17.4               186\n#>  3 Adelie      Adelie  Torgersen           40.3          18                 195\n#>  4 Adelie      Adelie  Torgersen           NA            NA                  NA\n#>  5 Adelie      Adelie  Torgersen           36.7          19.3               193\n#>  6 Adelie      Adelie  Torgersen           39.3          20.6               190\n#>  7 Adelie      Adelie  Torgersen           38.9          17.8               181\n#>  8 Adelie      Adelie  Torgersen           39.2          19.6               195\n#>  9 Adelie      Adelie  Torgersen           34.1          18.1               193\n#> 10 Adelie      Adelie  Torgersen           42            20.2               190\n#> # ℹ 334 more rows\n#> # ℹ 2 more variables: body_mass_g <int>, sex <fct>\n\nThe function works for most databases, but for technical reasons doesn’t work with all. It has been confirmed to not work work in spark databases or arrow tables.\naugment(orbital_obj, penguins_sqlite)\n#> # Source:   SQL [?? x 8]\n#> # Database: sqlite 3.47.1 []\n#>    .pred_class species island    bill_length_mm bill_depth_mm flipper_length_mm\n#>    <chr>       <chr>   <chr>              <dbl>         <dbl>             <int>\n#>  1 Adelie      Adelie  Torgersen           39.1          18.7               181\n#>  2 Adelie      Adelie  Torgersen           39.5          17.4               186\n#>  3 Adelie      Adelie  Torgersen           40.3          18                 195\n#>  4 Adelie      Adelie  Torgersen           NA            NA                  NA\n#>  5 Adelie      Adelie  Torgersen           36.7          19.3               193\n#>  6 Adelie      Adelie  Torgersen           39.3          20.6               190\n#>  7 Adelie      Adelie  Torgersen           38.9          17.8               181\n#>  8 Adelie      Adelie  Torgersen           39.2          19.6               195\n#>  9 Adelie      Adelie  Torgersen           34.1          18.1               193\n#> 10 Adelie      Adelie  Torgersen           42            20.2               190\n#> # ℹ more rows\n#> # ℹ 2 more variables: body_mass_g <int>, sex <chr>\n\nAcknowledgements\n  \n    \n      \n\n      \n\n    \n  \n\nA big thank you to all the people who have contributed to orbital since the release of v0.3.0:\n@EmilHvitfeldt, \n@joscani, \n@jrosell, \n@npelikan, and \n@szimmer.",
    "link": "https://www.tidyverse.org/blog/2025/01/orbital-0-3-0/",
    "pubDate": "Mon, 13 Jan 2025 00:00:00 +0000",
    "source": "Tidyverse Blog",
    "category": "R语言"
  },
  {
    "id": "tidyverse",
    "title": "Joining the ggplot2 team",
    "description": "5x height\n* [x] [`hugodown::use_tidy_thumbnails()`](https://rdrr.io/pkg/hugodown/man/use_tidy_post.html)\n* [ ] Add intro sentence, e.g. the standard tagline for the package\n* [ ] [`usethis::use_tidy_thanks()`](https://usethis.r-lib.org/reference/use_tidy_thanks.html)\n-->\nHello there! I’ve been working on ggplot2 for a while now, and I’d like to tell you how that came about and what it is like.\nHow I got involved\n  \n    \n      \n\n      \n\n    \n  \n\nMy journey into learning R started in 2017 during an internship at the EMBL-EBI. The main gripe about base R plotting that drove me into ggplot2’s arms were the arcane invocations to get anything else than one of the pre-approved chart types. In contrast, ggplot2 absorbs a bunch of small paper cuts, is very compositional in nature while remaining highly customisable. In a bid to “learn from the mistakes of others” rather than (continue to copiously) make my own, I became active on Stack Overflow answering questions and solving plotting issues. For posterity: this was in the days before you could ask an large language model for personalised advice and actual humans were equally frustrated on both sides of the question.\nI was keeping track of solutions to common problems in a personal cookbook that had its own arcane invocations. To give a bit of flavour: much of the cookbook was about preparing gtables (the data structure that comes out of building a plot) for combining and aligning plots. 1 The cookbook eventually grew into my first ggplot2 extension package: \nggh4x. Perhaps that package would be best subtitled: ‘Remedies to my common ggplot2 ailments’. It contains a bunch of miscellaneous functions ranging from reorganising facets to putting minor ticks on the axes. The nature of the package was also its downside, as ggh4x lacked any sense of scope (and still does, as befits any first package).\nAround the time when I was really getting into ggplot extensions, \nGina Reynolds had started organising a meeting for people who build ggplot2 extensions. It is an interesting place to meet others and hear about their packages and how they face interacting with the ggplot2 extension system. I started attending with some degree of regularity and made a discussion place on GitHub. We now use this for general exchange of ideas, but also package specific issues.\nMeanwhile, the questions on Stack Overflow kept directing my attention at the ggplot2 issue tracker every once in a while. After lurking in there for a bit, I started my first informal contributions to ggplot2 itself by answering the simple stuff just as I did on Stack Overflow. It may not seem like much of a contribution, but in retrospect, answering issues helps triaging them: it separates those issues that need additional changes in ggplot2 from those that do not. My first ‘proper contribution’ in the shape of a pull request was in 2020. It replaced 3 lines of code with 2 lines of code to benefit type stability (this was prior to \nvctrs)2.\nIn 2022, I commented “I’d be willing to take a stab at this” on an issue proposing a large refactor of the guide system. I like to think it was this precise moment that Thomas, the project lead after having taken over for Hadley, took notice and later invited me to join the team3. This new guide system ended up laying the foundation for \nlegendry, so it wasn’t entirely out of unselfish reasons that I volunteered. At any rate, this is a great opportunity to fill big shoes on a major R project, so I’m very excited to have joined!\nBecoming an insider\n  \n    \n      \n\n      \n\n    \n  \n\nPart of being on the team is straightforward. You triage issues. You fix bugs. You implement new features. At the point that I joined, I had already done these things as an outsider. The only thing that really changes is that you get the keys to the kingdom: you can now close issues and merge pull requests 4. You’re then trusted to wield this power wisely. You then hope you do.\nAt the time I joined the most active maintainers were Thomas, Claus and Hiroaki. I was surprised to learn that really most communication happens on GitHub and it is all public discussion. Even more abstract coordination that does not neatly fit into a single issue, like preparing a new release, didn’t occur behind closed doors. I think what made my introduction to the team more awkward than it needed to be was that GitHub issues is not really a good place for announcements where you can say ‘Hi everyone, this person is on the team now and will be doing stuff in the project’. I had interacted with the other active maintainers before, so I wasn’t a completely alien actor, but I felt some unclarity lingered longer than it ought have. Perhaps I should more assertively have introduced myself 5.\nHowever, by the time posit::conf(2024) was over, I’ve met 6 out of the 9 other authors in person. I have more thoughts about conf and my first time in the United States, but it has been amazing to meet all these people in person whose work you’ve been admiring for a while!\nMaintaining ggplot2\n  \n    \n      \n\n      \n\n    \n  \n\nThe ggplot2 package has both the blessing and the curse of being a popular package. One the one hand, it is a blessing that people care about the project, post issues that they find and make intermittent contributions. The curse is that it is such a staple in the R ecosystem, that almost any change will inadvertently affect somebody else’s code. Not only because ggplot2 is widely used, but also because people have been …creative… with how they are using ggplot2. The art of making changes is to largely affect plots in a good way.\nThe first big project I was rummaging through was the guide system I proposed to rewrite. The guide system had never been advertised as an official extension point, but naturally that didn’t preclude people from using it as an extension point anyway.6 So in addition to rewriting the system, we also had to prevent terribly breaking extensions that relied on the old system. In some cases, this meant sending out PRs to other packages to be compatible with both systems.\nHaving worked through a good number of issues at this point in time, I can see some emergent patterns. Different patterns can be partially explained by different audiences. The regular user wants to be empowered to execute their vision of a plot effectively. Maintainers of extensions would often like things to work consistently or change a very obscure line somewhere that they have identified as blocking a niche use case. Teachers would like their students to get stuck less often, which often involves improving error messages. All in all, there is no shortage of issues to work through.\nThe next big thing we’re working on is some practical necromancy in getting themeable aesthetics resurrected, which was \ninitiated by Dana Paige Seidel all the way back in 2018! We’d like the theme to be a home for more default choices than just non-data elements. Default layer aesthetics are a start, but we plan on putting in default palettes too.\nA few words of thanks\n  \n    \n      \n\n      \n\n    \n  \n\nI’ve been plucked from a level of relative obscurity —a package maintainer that has this weird miscellaneous package— into the path of a flagship R project, for which I’m very grateful. First and foremost I’m thankful to Thomas Lin Pedersen, who has put me into this position and steers the ggplot2 project. Secondly to Hadley Wickham and the rest of the tidyverse team, who make me feel included; both at conf and during regular meetings7. Thirdly, the co-authors I met during conf: Claus Wilke, for whose workshop I TA’d, but also Kara Woo and Winston Chang. Lastly, I’d like to thank Posit the company for contracting me to do work I also enjoy as a hobby!\nLuckily, we don’t have to think about this at all, thanks to the \npatchwork package! ↩︎\nI’m omitting here that I also had to write 50 lines of tests for this small change ↩︎\nHow much this actually reflects any truth is for any of us to guess and for Thomas to know. Later, I learned that this was also \nhow Thomas himself was roped into the project! ↩︎\nAfter review though. You’re not given that much power! ↩︎\nBut I’m not celebrated for my social graces :) ↩︎\nI don’t have a moral high ground here: I was one of the worst offenders! ↩︎\nMostly for The Golden Hex Sticker though! ↩︎",
    "link": "https://www.tidyverse.org/blog/2025/01/joining-ggplot2/",
    "pubDate": "Thu, 09 Jan 2025 00:00:00 +0000",
    "source": "Tidyverse Blog",
    "category": "R语言"
  },
  {
    "id": "tidyverse",
    "title": "tidymodels Internship for 2025",
    "description": "We are chuffed once again to offer a summer internship with the tidymodels team.\nWe’ve had eight previous summer interns and these led to the creation of a number of new packages: \nagua, \napplicable, \nbundle, \nbutcher, \nshinymodels, \nspatialsample, and \nstacks. Our own \nSimon Couch is a former intern who won \nan award for his work.\nThis year, the primary focus is on expanding our feature selection capabilities. Some of this will involve new recipe steps and other functions. Towards the end of the internship, there might be time to work on other things, too!\nTo apply, make sure that you have a GitHub handle and follow this link:\n\nhttps://posit.co/job-detail/?gh_jid=6323043003\nThe internship is US-based.\nIf you want to know what the internship is like, a few of our alumni have written about it:\nA summer with RStudio (2018)\nRStudio Summer Internship (2018)\nThis Is Not Like the Others (2019)\nTidymodels Internship (2020)\nI know what I did last summer (2022)\nWe can’t wait to get started and look forward to reading your applications.",
    "link": "https://www.tidyverse.org/blog/2025/01/tidymodels-2025-internship/",
    "pubDate": "Wed, 08 Jan 2025 00:00:00 +0000",
    "source": "Tidyverse Blog",
    "category": "R语言"
  }
]